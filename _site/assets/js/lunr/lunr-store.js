var store = [{
        "title": "Quotes",
        "excerpt":"Quotes  These are some of the most interesting short pieces of language I’ve heard throughout my life so far:   “Your mind is for having ideas, not holding them” - David Allen     “Be the change you wish to see in the world” - Ghandi     Together is better     “The network is robust in its unstructured simplicity” - Satoshi Nakamoto     “and tonight the stars’ll be out, and don’t you know that God is Pooh Bear?” - Jack Kerouac     “No input, no output” - Andrew Huberman     Awe is the creation of community     “If you have to ask how much it costs, you can’t afford it.” - J.P. Morgan     “We balance humility with our desire to better ourselves” - Jay Thompson     “Reading maketh a full man; conference a ready man; and writing an exact man.” - Sir Francis Bacon     “By three methods we may learn wisdom: first, by reflection, which is noblest; second, by imitation, which is easiest; and third, by experience, which is bitterest.” - Confucius     “The future you is counting on the present you to keep the promises you made to yourself yesterday” - Jordan Ferrone     The best investment you can make is in yourself     “Everything lies behind vulnerability” - Kaleb Joseph     Expectation is the thief of joy     A poor craftsman blames his tools     “Action expresses priorities” - Ghandi     The only constant is change     “You show me the man, I’ll show you the crime.”     “Mobility is life-changing” - Ben Floyd     “I can close off the paralysing thoughts, not by answering them on their own terms, but by realising that the choices I make today affect the kind of person I’ll be for the rest of my life.” - Neel Nanda      ","categories": ["saved"],
        "tags": ["quotes"],
        "url": "http://localhost:4000/saved/quotes/",
        "teaser": null
      },{
        "title": "Places with Ideas",
        "excerpt":"My Blog/Interesting Ideas Recommendations     Hackernews   Paul Graham’s Essays   Sam Altman’s blog   Vitalek’s blog   Neel Nanda’s blog   Jake Tae’s blog   Gwern  ","categories": ["recommendations"],
        "tags": ["resources"],
        "url": "http://localhost:4000/recommendations/other-blogs/",
        "teaser": null
      },{
        "title": "Grad Speech",
        "excerpt":"Crossroads  Welcome to the end. Welcome to the beginning. As we graduate, let’s reflect on the past, so that we can chart a course into the future.   Our time at Bangor High School has given us many gifts. Knowledge is power, and our studies have shown us: the history of the world thus far, how to communicate effectively—both in our own language and in others, how make our ideas reality through the mediums of art, technology, and music, how to appreciate and apply the beauty of the multiverse through math and the sciences, and how we can better ourselves and our society in this ever changing world.   Time moves fast. Almost unbelievably so. Time can feel agonizingly slow in the moment, but in hindsight—fleeting. Four years ago we started together. Now we finish together. As a class we faced the unprecedented turbulence of Covid-19 and its slew of related side-effects: remote school, quarantining, and general uncertainty.   But this is the world we live in, And these are the hands we’re given, so as we reflect on where we’ve come and where we’ll be going, now is a valuable time to pause and really ask yourself some important questions. What are your goals? What do you want to do with your life? These are questions whose self-guided answers are extraordinarily valuable, yet we rarely find the time for introspection. Day to day life is fast. You blink and it’s next week. You blink again and you’ve graduated high school. Maybe the next time you blink, you’ll be a Bitcoin millionaire. But finding out what we want to do with the incredible gifts we have, the gifts of life, of youth, of education, of privilege, is powerful. Realize how much potential we have. But with great power comes great responsibility. And we have the power to determine the future of everything.   Facing the great challenges of our time: climate change, pandemics, fighting for democracy, sustainable energy generation, is either a duty, a destiny, or an opportunity, depending on how you look at it. Everything is up to you! You are the one who will do what you want to do. No one else can live your life for you. The only thing separating where you are now from where you want to be is doing the WORK to make it a reality.   Today I see the people I’ve had the incredible fortune of sharing countless experiences with. I see the future paths diverging into the great, vast unknown. Right here, right now, there is no other place I want to be. We are poised to change the world. Actually, we change the world regardless of what we do. The only constant is change.   I would like to thank Douglas Adams, Elon Musk, Jack Kerouac, Satoshi Nakamoto, and of course everyone else who has ever lived. We stand on the shoulders of giants. We will become those giants.   And when you get the chance to sit it out or dance, I hope you dance! For well you know that it’s a fool who plays it cool By making their world a little colder.   So long, and thanks for all the fish.  ","categories": ["speaking"],
        "tags": ["me"],
        "url": "http://localhost:4000/speaking/gradspeech/",
        "teaser": null
      },{
        "title": "Melvin Krulewitch Military Career Overview",
        "excerpt":"Melvin Krulewitch Military Career Overview  Written under the tutelage of Viktor Shmagin (and my dad).  ","categories": ["writing"],
        "tags": ["history","family"],
        "url": "http://localhost:4000/writing/melvin-krulewitch-military-career/",
        "teaser": null
      },{
        "title": "Japan Blogs",
        "excerpt":"My Princeton in Ishikawa (PII) and Japan-America Student Conference (JASC) Blogs  I wrote these blogs to fulfill the requirements of my fellowship funding (but they were fun to write, good for reflection, and are now nostalgic to look back on). My blogs from PII are definitely better than my JASC ones.   JASC76 Report  LA, New Orleans, and DC. From August 1st to August 24th, 2024, the 76th Japan America Student Conference (JASC) flew, drove, trollied, walked, and rode through these iconic American cities. I am immensely grateful to have been a part of this conference - the friends, the interpersonal skills, and the countless memories stand out as my biggest takeaways from this year’s JASC. If you’re thinking about applying, please do (I almost didn’t).   Here’s the gist of what JASC is: the world’s longest-running college-student-led conference, initiated in 1934 by university students concerned by the breakdown of bilateral relations prior to the Second World War. Its mission is to promote peace by furthering mutual understanding, friendship and trust through international student interchange. JASC alternates its host country every year (between Japan and America), with ~30 American students and ~30 Japanese students organized into 7 different roundtables. The roundtables discuss their topic and create presentations based on their discussions, culminating at the Final Forum at the end of the conference.   My JASC interview went well. It was over Zoom and the notes I prepared proved helpful as I glanced at them during the Q&amp;A. I joined the Culture, Arts, and Technology roundtable (CAT). I’m interested in AI and cryptocurrencies, so I hoped to focus on the T in CAT during the conference. Looking back, now that it’s over, I’m glad to have seen, heard, and respected my roundtable peers on their experiences and interests in the all-encompassing “topic” of Culture, Arts, and Technology. Their ideas helped me see the world in a different way and better appreciate design and creative expression.   Before the conference, the American side of our roundtable (three people, including me) met regularly on Zoom to discuss articles related to our interests. A last-year’s-delegate-turned-Executive-Committee-member (an EC) organized the meetings and acted as our first point of contact with the wider conference. We talked to our Japanese counterparts (also a part of the CAT roundtable) on a Zoom once, but we joined forces completely when we met at the HI Hostel Santa Monica in Los Angeles.   I had been in Tokyo the previous month, living in a sharehouse, interning at an AI startup, and meeting interesting people. So I flew straight from Haneda to LAX (a day before the conference started so I could get my UK visa for WEPO) and started my eastward journey back to the Pine Tree State. I love hostels, and JASC does too. We stayed at places with bunk beds and shared bathrooms and wash-your-own-dishes vibes with nice spaces to hang out or do work.   American Orientation, 3 days to get to know each other before the Japadeles arrived, began in Santa Monica on August 1st. We worked on our first roundtable presentations and shared them at USC; I presented on the Llama 3.1 paper and Stable Diffusion. Then the Japadeles arrived and the hostel room (4 bunk beds, 8 people, in a small room) filled with suitcases, clothes, and delegates.   In Los Angeles, we visited Little Tokyo, the Huntington Gardens (where we participated in an Urasenke tea ceremony), the Los Angeles LGBT Center, the Bergamot Galleries, Venice Beach, and USC (where we gave our Midterm Forum presentations). At the Bergamot Galleries, I argued with a member of our roundtable. My thoughts were that we should practice voting on things we want to do as a group. Their thoughts were that voting would be a waste of time and we could come to consensus through discussion instead. This dispute resulted in strained relations for a few days, but I remained friendly with the other person and they warmed back up to me. I practiced not taking things personally, realized that our disagreement stemmed from misunderstanding, and now better appreciate the wisdom of the Art of War quote “the greatest victory is that which requires no battle.” We worked well together for the rest of the conference. In our Midterm forum presentation, our roundtable discussed copyright law and artists’ rights issues in GenAI. Our EC leader, a food illustration artist, agreed to let me use her art so I presented a photo-of-food-turned-illustration-in-her-style using code from Professor Hopkins’ Deep Learning style transfer lab. This real-world example helped us discuss the implications of style transfer and artists’ rights. That night we had pizza on the beach and prepared to leave early the next morning for New Orleans.   My fondest memory from LA, and perhaps the entire conference, was that night right before we left. My cousin, coincidentally, was also in LA so we met at the Santa Monica beach and joined the other JASCers playing beach volleyball. Then, with a group of American and Japanese delegates, we walked over to the Santa Monica pier, tried the “Japadog” (a food stand selling Okonomiyaki/Wagyu hot dogs that I brought up unceasingly because I thought it was so funny - but turned out to be delicious), played classic American boardwalk games like Whac-A-Mole, Air Hockey, and Skee Ball at Playland Arcade, and rode the roller coaster. I’ll cherish these fun memories with my newly-made friends forever.   One thing I noticed about the conference sites, as someone from a Maine “city” and Williamstown, were the homeless people. The Japadeles (not Amedeles) had to attend safety lectures - and for good reason. At the Japanese American National Museum in LA, I discussed with my Japadele friends about the differences in history education in Japan and America, the internment camps, and anti-Japanese sentiment in the states. As we rode the train back, a woman (perhaps on drugs) told the Japanese delegates to “go back to their country” which felt surreal and was a blatant reminder that history isn’t just a thing of the past.   In New Orleans, our first stop was the National WWII museum. My great-grandfather was a lieutenant colonel in the Pacific theater and I took a history tutorial about the Asia-Pacific War last fall, so I was well-equipped to appreciate the artifacts, exhibits, and oral histories in the museum. The Japanese delegates were impressed with the museum’s neutrality - my friend told me that the history museum next to Yasukuni Jinja (where top convicted Japanese war criminals are enshrined) paints Japan as a victim of American aggression. The next day we visited the Historic New Orleans Collection, a museum about incarceration in Louisiana (the nation’s highest rate) and how slavery has changed forms but exists to this day as prison labor. Then we experienced traditional New Orleans Jazz at the Toulouse Theatre and ate beignets at Café Du Monde. On our free day, I joined a group of Japanese delegates on a bayou alligator-spotting tour. After the tour, we had lunch with my friend from high school who goes to Tulane, and I think it was great for both me and the Japadeles to learn about what she’s doing and the strained relationship between the Tulane student body and the citizens of New Orleans. That night, I led a productive roundtable meeting where we chose to focus on the intersections between the music industry, Japan-US relations, and technology. Our final day in New Orleans was a reflection forum where we enjoyed mixed roundtable discussions about our experience so far. We also heard from Susie Allen, a JASC alum, about Hurricane Katrina and from Harrison Crabtree, the Director of the World Trade Center New Orleans, about how the gaming industry and NASA are contributing to the city’s diversified and resilient economy.   In DC, the last stretch of the conference began and the days and nights started blending together into one long continuous blur. During JASC I slept the least I’ve ever slept in my life. The friends and the experiences continue long into the night. The first night, we walked down to the White House and I had a long conversation with Shimons about Bitcoin, US dollar hegemony, the future of AI, startups and venture capital in Japan, and how we will seek out and learn from cutting-edge researchers at Oxford and the University of Tokyo. The second night, August 15th, was a special day. In 1945, it was the day Japan surrendered to the Allies, effectively ending World War II. This year, not only was it my 21st birthday and the 90th anniversary of the first JASC, but it was also the 21st birthday of my friend Shimons (Takahito Shimoonoda) - a Japanese delegate in my roundtable who, like me, is interested in AI and finance! For the 90th anniversary, there was a reception, dinner, speeches, and networking between JASC alums, donors, and current members. The next day, we visited the IMF. I was impressed by their mission of macroeconomic stability and their clear reasoning about the future: climate change and geopolitical fragmentation are the two biggest threats to world prosperity. For our roundtable field trip, we visited ARTECHOUSE, a digital art experience similar to teamLabs. The current exhibition is Isekai (異世界) which means other world or parallel universe - a manga/anime trope. The immersive art demonstrated how technology and art can co-exist and augment each other; exhibits included an immersive projection room flying through fantasy worlds and a place to chat with an AI avatar with its own life story. In subsequent days, we visited Georgetown and prepared for the Final Forum. Our presentation addressed two topics: the dangers of cultural echo chambers and the question of can AI replace human artists. We also covered major differences between Japan and America’s music market (CDs vs streaming), fandom culture, album cover art, Spotify DJ, and Suno’s AI generated music. I presented a little app I made (open source at https://github.com/ssocolow/CAT-Spotify-App) that uses Spotify’s API to recommend and play songs from a randomly selected country’s market - to combat geopolitical fragmentation by exposing listeners to different cultures. The day after the Final Forum we elected next year’s ECs (I chose not to run), and the day after that I went on a run with my high school friend on the Georgetown XC/Track team and Shimons and I visited the Library of Congress, the Supreme Court, Union Station, and the Smithsonian National Air and Space Museum. The next day we said our goodbyes and I rode Amtrak to the North.   In DC - and around the United States - I kept noticing connections to Japan. Some were subtle like “NTT DATA” spray-painted onto the Washington Nationals baseball field and some were obvious like the Japadog stand. JASC helped me notice the depth and breadth of the US-Japan relationship - you can find it anywhere if you look hard enough. And JASC itself has strengthened the bonds between our two countries, through people-to-people diplomacy and personal connections that last lifetimes.   I would like to express my deep gratitude for the Williams College Japanese Program and the Department of Asian Languages, Literatures, and Cultures for generously funding my experience.  ","categories": ["blog"],
        "tags": ["japanese"],
        "url": "http://localhost:4000/blog/pii-blogs/",
        "teaser": null
      },{
        "title": "The Reason Japanese Watch Television",
        "excerpt":"日本人がテレビを見る理由  こんにちは、私のスピーチは日本のテレビについてです。 ホストファミリーの家で、毎日、テレビを見ます。朝と夜と、いつもテレビを見ながら、料理を食べます。アメリカで、テレビをあまり見ません。スーパーボールだけ見ます。PIIの友達によると、他のホストファミリーの家でも、いつもテレビを見るそうです。これは面白いから、どうしてかなと思ったので、母に質問をしました。母は、「テレビを見るのはよくないんですが、子供の時からに見ています」と言っていました。それで、日本人のテレビを見る理由を調べ始めた。 毎日、午前七時に、”THE TIME”というチャンネルで、テレビ体操があります。テレビ体操はラジオ体操みたいです。テレビ体操を見ながら、テレビの人といっしょに、見ている人も踊ります。テレビ体操の次はたいてい、おおたにしょうへいのホームラーンのニュースです。 日本人はかわいいことが大好きみたいです。たとえば、テレビ体操が始まる時に、かわいい鳥が現れて、踊ります。私の意見は日本のテレビは時々かわいすぎです。日本の社会の文化はきびしいから、かわいくて、完璧なことはちょっと本当じゃないんじゃないかと思います。日本の社会の中で、時々、本当の気持ちを言えないから、日本人はもっと正直にした方がいいと思います。 交流会で会った女の人によると、テレビを見る理由はニュースです。ところが、ニュースは短いし、スマホのネットのニュースの方がいいし、この理由は一番大切なテレビを見る理由じゃないと思います. 私の仮説はテレビを見る時、社会の中にいる気持ちがあります。テレビの世界の中で、色々なおいしい食べ物の番組が見られるし、楽しくて、面白いゲームの番組もあるし、自分が社会にいる気持ちになれるのかもしれません。 私は、アメリカで、テレビを見ません。でも、日本のテレビはぜったいとくべつだと思います。日本のテレビを見る時に、体操とニュースと社会にいる気持ちがありますから、たしかに、テレビを見るということは、日本の文化といえるのではないかと思います.  ","categories": ["speaking"],
        "tags": ["japanese"],
        "url": "http://localhost:4000/speaking/the-reason-japanese-watch-television/",
        "teaser": null
      },{
        "title": "Bitcoin Starter",
        "excerpt":"Beginner Friendly Bitcoin Resources  A notion page I made for people interested in learning about Bitcoin.  ","categories": ["cryptography"],
        "tags": ["bitcoin","info"],
        "url": "http://localhost:4000/cryptography/bitcoin-starter/",
        "teaser": null
      },{
        "title": "Tech Tips For Learning Japanese",
        "excerpt":"Tech Tips For Learning Japanese  A notion page I made with tips on using tech for learning Japanese.  ","categories": ["recommendations"],
        "tags": ["japanese","info"],
        "url": "http://localhost:4000/recommendations/japanese-tech-tips/",
        "teaser": null
      },{
        "title": "Group Theory Explainer",
        "excerpt":"Group Theory  I learned group theory from Misha for Michaelmas term and his last assignment was for us to create a document to help us remember what we’ve learned in the course. I think this final assignment was helpful because it forced me to create a cohesive big-picture view of the subject as a whole. The original latex is here, this markdown was generated with pandoc.   Groups   Definition of a Group: Let $G$ be a set and $  *  $ be a binary operation. Then $(G,   *  )$ is a group if:           $  *  $ is a associative: so if $g_1,g_2,g_3 \\in G, g_1 * (g_2 * g_3) = (g_1  *  g_2)  *  g_3$            $G$ contains an identity element (which I will denote as $e$): there is $e \\in G$ where for any $g \\in G$, $g  *  e=e  *  g=g$            every element of G has an inverse: if $g \\in G$, there exists $g^{-1} \\in G$ where $g  *  g^{-1}=g^{-1}  *  g=e$            $G$ is closed under $ * $ : for any $g_1, g_2 \\in G, g_1 * g_2 \\in G$       If $G$ is finite, we write $|G|$ for the number of elements in $G$. Note that people usually write $g_1g_2$ to mean $g_1 * g_2$.   Examples   The integers under addition is a group with $e = 0$ and $g^{-1}=-g$. The cyclic group of two elements, $C_2$, is a group ${e,a}$ where $aa=e$. $C_3$ is a group ${e,a,a^2}$ where $a^2 = aa$ and $a^3=e$. In general, the cyclic group $C_n$ is like a clock with $n$ points (and is the “same as\" = isomorphic to the integers mod n). There are other cool groups like the symmetric group, dihedral group, the free group, ... I would like to remember that taking time to play around with groups is generally worth it - seeing how they interact makes it easier to reason about them.   Homomorphism   A map of groups $\\phi: (G_1, \\ast) \\to (G_2, \\circ)$ is called a  homomorphism  if it preserves the group operation: [phi(g * h) = \\phi(g) \\circ \\phi(h) \\quad \\text{for all } g, h \\in G_1]   I found concrete examples of homomorphisms hard to find so here is one: the homomorphism defined from $\\mathbb{Z}$ to $\\mathbb{Z}\\text{ mod 3}$ as $0+3n\\xrightarrow{}0, 1+3n\\xrightarrow{}1,2+3n\\xrightarrow{}2$ where $n$ is an integer.   Isomorphism   One important question in group theory is: given two groups A and B, is A the “same\" as B? One great definition of “sameness\" of two groups is an isomorphism. An isomorphism is a map between two groups that preserves their structure and is also reversible. Formally, an isomorphism is a bijective homomorphism. Remember that “bijective\" means both injective (one-to-one) and surjective (onto).   Conjugation   Two elements $h,k \\in G$ are  conjugate  in $G$ if there exists $g\\in G$ such that $h=g^{-1}kg$. Conjugation of an element $k$  * by *  an element $g$ gives $g^{-1}kg$. If $b=g^{-1}kg$ then $b$ and $g$ are in the same conjugacy class. I found conjugacy classes hard to wrap my head around. Conjugacy classes partition a group, and we can see this with $D_6={e,r,r^2,s,sr,sr^2}$, the symmetries of the triangle under rotation and reflection. The rotations ${r,r^2}$ are a conjugacy class because if we conjugate a rotation by any rotation we will end up with a rotation and if we do so with any reflection we will end up reflecting twice so also end up with a rotation.   Subgroups   Basically, a subgroup is a group that exists inside a group. Definition: $(H, * )$ is a subgroup of $(G, * )$ if $H \\subseteq G$ and $(H, * )$ satisfies the properties of a group. We write $H \\leq G$.   Lagrange’s Theorem   If $G$ is finite and $H \\leq G$, then $|H|$ divides $|G|$. For example, one subgroup of the cyclic group $C_6$ is $C_2$ because ${e,a^3}$ is a subgroup of ${e,a,a^2,a^3,a^4,a^5}$ and 2 divides 6.   Normal Subgroups   A group $N$ is normal in $G$ if for every $n\\in N$, conjugation by any $g\\in G$ gives an element of $N$. If $N$ is normal in $G$, we write $N \\triangleleft G$. Normal subgroups are important because they allow for the creation of groups at another level of abstraction - quotient groups.   Quotient Groups   Quotient groups can be really confusing. I think that’s because I had a hard time visualizing them and how they work. To start, let $G$ be a group and $H$ be a subgroup of $G$. $G/H$ is the set of (typically left) cosets of $H$ in $G$. So, $G/H = {gH | \\forall g\\in G}$. Here, $gH$ means take the element $g \\in G$ and make a set containing the result of the group action of $g$ with every element of $H$. There might be some $g_1,g_2 \\in G$ where $g_1H=g_2H$. You can think of the quotient $G/H$ as “modding out\" from $G$ all the elements $g_1,g_2,…$ where $g_1H=g_2H=…$ so that you are left with one element in $G/H$ that represents $g_1,g_2,…$. Example below. If $N$ is a  * normal *  subgroup in $G$, then $G/N$ is a group with group operation $g_1N  *  g_2N = (g_1g_2)N$.   Example   The quotient group $\\mathbb{Z}/8\\mathbb{Z}$ is the integers mod 8 = ${0,1,2,3,4,5,6,7}$. $8\\mathbb{Z}$ are the multiples of 8 under addition, so $8\\mathbb{Z} ={…,-16,-8,0,8,16,…}$. If we take any $g \\in \\mathbb{Z}$ (so any integer $g$), then $g(8\\mathbb{Z}) = {…,-16+g,-8+g,0+g,8+g,16+g,…}$. We can see that $0(8\\mathbb{Z})=8(8\\mathbb{Z})$ because ${…,-16+0,-8+0,0+0,8+0,16+0,…} = {…,-16+8,-8+8,0+8,8+8,16+8,…}$.   First Isomorphism Theorem   If $\\phi:G\\xrightarrow{}H$ is a homomorphism between groups then the kernel of $\\phi$ is $\\ker\\phi={g\\in G: \\phi(g)=e_H}$. So the kernel is what elements in $G$ that $\\phi$ sends to the identity in $H$. The image of $\\phi$ is the set of elements in $H$ that $\\phi$ can send elements in $G$ to. So Im$\\phi={\\phi(g):g\\in G}$. The First Isomorphism Theorem tells us that $G/\\ker\\phi$ is isomorphic to Im$\\phi$ by the map $g\\ker\\phi\\rightarrow{\\phi(g)}$. I didn’t understand why this was important, but Peter helped me see that $G/\\ker\\phi$ can be thought of as “things in $G$ that don’t map to the identity\" which is a foundational fact about group structure which I would like to remember.   Free Groups   Free groups are the “most free\" kind of group because they have the fewest restrictions on what elements are allowed to exist. There is a theorem that any group is isomorphic to the quotient of a free group. Definition: For any set $S$, the free group $F(S)$ are all the “reduced words\" that can be created from the “alphabet\" $S$ (the alphabet includes inverse characters). So if $S={a,b}$ then $aa$ is a word, $ab$ is a word, $ba$ is a word, and $a^{-1}b$ is also a word. The group operation is concatenation, so $ab * ba=abba$. A word is “reduced\" if no element is next to its inverse, so $aa^{-1}bab$ reduces to $bab$.   Having the background of computer science made me feel comfortable with free groups because I could think of the free group elements as variables representing strings and I know how string concatenation works (just have to remember to simplify, e.g. $baa^{-1}=b$). I’d like to remember that any group can be thought of as a free group with some restrictions imposed. That segues nicely into   Presentations   Presentations are just another way to define a group. When I talk about $D_8$, the group of symmetries of the square under reflection and rotation with the group operation being composition of those actions, I can tell you the elements of $D_8$ are ${e,r,r^{2},r^{3},s,sr,sr^{2},sr^{3}}$ where $r$ means a rotation by $\\frac{\\pi}{2}$ and $s$ means a reflection about the vertical axis or I could tell you that there are two generators $r$ - a rotation - and $s$ - a reflection - , where $r^4=s^2=e$ (because rotating 4 times gets you back where you started and flipping twice does too) and $srs=s^{-1}$. So, I have the free group on the set ${r,s}$ but there are restrictions on what elements can exist. In the notation, I would write $D_8=&lt;r,s|r^4=e,s^2=e,srs=s^{-1}&gt;$. I struggled with identifying a group from it’s presentation - even if it was obvious!. I’d like to remember that anything familiar can be made unfamiliar if you view it from a different perspective.   Jordan-Holder Theorem   A group can contain nested subgroups, and one of the biggest questions in group theory is how we can analyze a group by breaking it into smaller pieces to see the structure of the bigger group. A finite group $G$ has a  composition series  ${e}\\triangleleft G_1 \\triangleleft … \\triangleleft G_n=G$ where $G_1,…,G_n$ are subgroups of $G$. The Jordan-Holder Theorem (JHT) is very similar to the fundamental theorem of arithmetic (the prime-factorization theorem: “every integer greater than 1 can be represented uniquely as a product of prime numbers, up to the order of the factors\"). What the JHT tells us is: if $G$ is a finite group, all composition series of $G$ have the same length and the same composition factors, in some order. So you can kind of think that the composition factors for the group are like the prime factors for a number (but there can be different groups with the same composition factors). For an example, we can see that the composition factors of $D_8$ are $C_2,C_2,C_2$. We can write the composition series as ${e}\\triangleleft C_2 \\triangleleft C_4 \\triangleleft D_8$ or ${e}\\triangleleft C_2 \\triangleleft D_4 \\triangleleft D_8$ where $D_4=C_2\\times C_2$. I didn’t realize I could put $D_4$ in the composition series for $D_8$ on week 5’s problem set. I’d like to remember that the same group composition factors can join into a bigger group structure in different ways - that $C_8$ has the same composition series as $D_8$ but they are DIFFERENT groups.   Classification of Finite Simple Groups   Simple groups are groups with no proper (different than the original group) nontrivial (different than ${e}$) normal subgroups. A landmark achievement in group theory was the classification of finite simple groups. Humanity now knows which exact group types are the building blocks for ALL finite groups (because of the Jordan-Holder Theorem). There are some crazy finite simple groups, like the Monster.   Semi-Direct Products   First, what is a direct product? A direct product combines two groups and forms a new group, but the elements from the two groups don’t interact. For example, $\\mathbb{Z}\\times\\mathbb{Z}={(a,b):a,b\\in\\mathbb{Z}}$. So the direct product of the integers with itself gives the group of points on the Cartesian (integer) plane. And $(a,b) * (c,d)=(a * c,b * d)$. Semi-direct products are more interesting because they let elements in both groups interact. To define a semi-direct product, it will be helpful to know what an automorphism is. An automorphism is an isomorphism from a group to itself. The identity automorphism sends every element back to itself, and a non-identity automorphism basically relabels elements but crucially maintains the structure of the group (so the identity element is always sent to itself). If we have two groups $H$ and $N$, and a homomorphism $\\phi:H\\xrightarrow{}Aut(N)$, we can define $(N \\rtimes H, \\circ)$ to be the set of pairs $(n,h):n\\in N,h\\in H$ with the group operation $(n_1,h_1)\\circ(n_2,h_2)=(n_1\\phi(h_1)(n_2),h_1,h_2)$. $\\phi(h_1)$ is an automorphism of N, so it will relabel $n_2$ to be another (or the same) element in $N$. I was confused whether the automorphisms of $N$ was the same as conjugation. I would like to remember that statements like the one I just made are ambiguous because “conjugation\" is not an action - “conjugation  * by *  an element is an action. Similarly, saying a group $N$ is normal doesn’t mean anything - the property of being normal requires a group $G$ to be normal  * in * . Big people also get confused with these specifics sometimes. In the case of automorphisms of a group $N$, there are inner automorphisms of $N$ (which are conjugations of elements of $N$  * by *  elements of $N$) and outer automorphisms (conjugation of elements of $N$ by elements outside the subgroup $N$). For the homomorphism $\\phi:H\\xrightarrow{}Aut(N)$, I think $\\phi \\text{ sends } h\\xrightarrow{}(n\\xrightarrow{}hnh^{-1})$.   Sylow’s Theorems   Given a group with a certain number of elements, what do we know about the subgroups that exist within this group? Lagrange’s Theorem tells us the order of a subgroup divides the order of the group. Ok, but wouldn’t it be nice to know  which  divisors of the group correspond to subgroups? To answer this question, Sylow’s Theorems are useful. If $G$ is a group with order $p^am$ where $p$ is prime and $p$ doesn’t divide $m$, we call a subgroup $H$ of $G$ a Sylow p-subgroup if $H$ has order $p^a$. For a finite group $G$ and prime number $p$, we can uniquely write $|G|=p^am$ where $a\\geq0$, $m\\geq1$ and $p$ does not divide $m$. Sylow’s First Theorem tells us that there is a Sylow p-subgroup of $G$. Sylow’s Second Theorem tells us that two Sylow p-subgroups are conjugate, so if there are multiple p-subgroups we can get between all of them through conjugation with elements in $G$. Sylow’s Third Theorem says $n_p$, the number of Sylow p-subgroups, is congruent to $1$ mod $p$ and $n_p$ divides $m$. I struggled using Sylow’s Theorems to show that a group of order $pq$ (where $p$ and $q$ are prime) was simple. I would like to remember that these theorems are useful to count the number of p-subgroups in a bigger group, and if that number is 1 then we know the bigger group is not simple because that p-subgroup is normal in the bigger group. A more general takeaway is that Sylow’s Theorems gives us insight into what kind of subgroups (order $p^a$) - and how many - exist within a finite group.  ","categories": ["math"],
        "tags": ["info","study"],
        "url": "http://localhost:4000/math/24-group-theory-explainer/",
        "teaser": null
      },{
        "title": "Visualizing ML Algorithms",
        "excerpt":"Visualzing Linear Regression, Logistic Regression, and SVMs  One of my Hillary term tutorials was Data Visualization, where I learned from Hector a few things:     keep in mind what story you want to tell   tell the story through the visualization   make it easy for the audience to grasp your point   For my final project, I made this visualization of ML algorithms. Check it out!  ","categories": ["ML"],
        "tags": ["study"],
        "url": "http://localhost:4000/ml/data-vis-ml/",
        "teaser": null
      },{
        "title": "Public baths",
        "excerpt":"Public baths  I’ve had the good fortune to experience public baths in many different cultures. In all of them, relaxation and being present with the moment have been key.  ","categories": ["blog"],
        "tags": ["travel"],
        "url": "http://localhost:4000/blog/public-baths/",
        "teaser": null
      },{
        "title": "Sleeping in airports",
        "excerpt":"This is a story about a travel hack. A dangerous travel hack. A travel hack I don’t recommend.   Staying one night in an airport saves one night’s worth of housing/hotel/hostel costs. But it comes at a steep price - bad/no sleep.   I tried it on my recent trip to Barcelona. My plane landed at 2am so I ‘slept’ the rest of the night in the terminal before exiting to find my hostel where I slept the next two nights. My flight out of Barcelona boarded at 6:20am, so instead of waking up at like 3:30am that morning I got to the airport around 10:30pm the night before, scanned in to my terminal with my boarding pass (which I think allows passengers into their terminals 24hrs before their flight), and found a cozy cushioned bench to lie down on and used my backpack as a pillow.   My friend inspired me to try this out by showing me sleepinginairports.net which had bad ratings for Barcelona’s airport but I was undeterred. A different friend warned me against doing it, they had a miserable experience in a Portugal airport.   The first night went worse than my second. I didn’t get a good spot - only the hard rubber of a child play area and then a rigid glass bench. I knew where to go and where to avoid on my second night, and found a cushioned bench at a Pret that worked well (which had been full of other passengers on my first night).   Tips     Bring a good sleeping mask and earplugs   Explore the area and try to find cushioned benches   Expect to get very poor sleep   Bring something that works well as a pillow - try for something better than clothes / a bag with clothes in it   Conclusion  In the end, do I regret sleeping two nights in the airport? Yes, I think I do. One night would have been enough: I knew what it felt like and knew that feeling tired after lying in an uncomfortable position for hours is unpleasant and unhealthy. Actually, I could have come to this conclusion without sleeping in the airport at all - my curiosity got the best of me.  ","categories": ["blog"],
        "tags": ["travel"],
        "url": "http://localhost:4000/blog/sleeping-in-airports/",
        "teaser": null
      },{
        "title": "LLMs Can Be Good",
        "excerpt":"   This essay is my first essay for my Ethics of AI philosophy tutorial (under the tutelage of Benjamin Lang). The assigned readers were written no later than 2020, and I felt that a reexamination of past arguments was well justified, given ChatGPT and the astounding advances of modern LLMs since then. During the tutorial, Ben brought up that if LLMs can be good (meaning they are moral agents) then we have ethical responsibilities to them, like maybe turning them off is problematic. He also brought up that blame is a two-way street - if we can blame an LLM, for example because it didn’t do its duty, then they can also blame us.    The experience of interacting with modern large language models (LLMs) yields valuable evidence for evaluating arguments about machine morality. Sven Nyholm poses the question “Can Robots Be Good?” in 2020 and concludes “No” with moderate confidence (Nyholm 2020, 166-169). Since 2020, the world has changed in profound ways: machines pass the Turing test with flying colors and play an increasing role in everyday life as AI-powered search summaries, remarkable coding assistants, and helpful chatbots. The recent profound changes in AI capabilities prompt a reevaluation of past arguments about whether robots can be considered moral agents. This paper reevaluates Nyholm’s arguments in light of what we know now and makes the case that modern LLMs may be considered moral agents. The first section identifies statements from past arguments against machine morality and refutes them with new evidence - leading us to question their conclusions. The second section is an attempt to use modern LLMs to satisfy Nyholm’s assembled criteria for being “good” which fall under two main theories: virtues and duties. In doing so, this paper argues that modern LLMs may be moral agents.   Impact of New Evidence on Past Arguments  Many past arguments against the statement “robots can be good” rely on statements that new evidence strongly disagrees with. Moor, for example, says “computers today lack much of what we call commonsense knowledge … This is probably the most serious objection to robot ethics” (Moor 2009). Computers today have commonsense knowledge - we invite the reader to visit chat.com to verify for themselves. Moor’s statement was true when it was first written but is now false - robust evidence now exists that challenges this “most serious” objection to robot ethics.   In “Incorporating Ethics into Artificial Intelligence”, Etzioni et al. focus on George Lucas, Jr.’s distinction between machine autonomy and moral autonomy. He uses the example of a Roomba or a Patriot missile which are mechanically autonomous but lack moral autonomy - they can’t change or abort their missions if they have moral objections (Etzioni et al. 2017). However, major LLM providers of today (OpenAI, Anthropic, Google, Deepseek) all have abort mechanisms built into their chatbot platforms - they will refuse to answer prompts involving harm, danger, and unethical behavior. Therefore LLMs are morally autonomous under Lucas’ definition.   Later in the article, Etzioni et al. conclude with “it seems that one need not, and most likely cannot, implant ethics into machines, nor can machines pick up ethics as children do, such that they are able to render moral decisions on their own” (Etzioni et al. 2017, 416). This statement considers the “bottom up” paradigm of learning, through which the gradient descent algorithm has led to stunning achievements. Peter Railton envisions machines learning ethics like humans: “We should be asking how systems … might come to be themselves complex social agents … learning fundamental elements of ethics … Just as infants observe countless hours of adult behavior” (Railton 2020, 66). Similarly, modern LLMs pre-train on vast datasets of text, becoming excellent at minimizing the difference between their expectation of reality and reality itself. Through pre-training and post-training, LLMs learn like children do by forcing them to confront reality countless times in order to build some model of the world that matches their experiences. This process is evidently successful. A recent study showed that “Americans rate ethical advice from GPT-4o as slightly more moral, trustworthy, thoughtful, and correct than that of the popular New York Times advice column, The Ethicist” (Dillion 2025).   This study is powerful evidence against some of the strongest arguments against machine ethics from Purves et al.. Nyholm reviews Purves et al.’s requirements for ethical behavior: ethics is contextually sensitive so requires the exercise of the capacity for judgement, and requires the capacity to act on the basis of reasons (Nyholm 2020, 158). The study shows that machines can be contextually sensitive in their responses - GPT-4o must be if it can be ranked higher than expert humans in giving ethical advice. The two theories of what it means to act on the basis of reasons are i) “acting on the basis of beliefs and desires”, and ii) “acting on the basis of a distinct mental state described as ‘taking something to be a reason’” (Ibid). Anthropomorphizing a bit, one might imagine an LLM acting on the “desire” to stay in a local minimum of a loss function by producing responses that align with its training data. Nyholm shields himself from debate by saying that both “ways of acting for a reason require a human mind and consciousness” and therefore robots cannot act for reasons, ergo they cannot act morally right or wrong (Ibid).   GPT-4o’s ethical advice scored a higher perceived quality of thoughtfulness than The Ethicist’s advice, demonstrating that it can reason effectively (Dillion 2025). This result seems to satisfy the properties that Moor uses to define an explicit ethical agent: “agents that can identify and process ethical information about a variety of situations and make sensitive determinations about what should be done” (Moor 2009).   In conclusion, the profound increases in AI capabilities allow us to examine past arguments with powerful new knowledge; the earthquake reveals which houses are built on a strong foundation.   How Robots Can Be Good  The following section is an attempt to fulfill the conditions of the two theories Nyholm discusses when he defines what it means to be “good”: having virtues (Aristotle and Hume) and doing duty (Mill and Kant) (Nyholm 2020, 154). This paper will not attempt to grapple with the questions of if AI can be conscious or if consciousness is required to be a moral agent. In addition, we will not attempt to untangle whether personhood is required to be a moral agent. In leaving these debates for now, we will attempt to have an open-minded investigation into how modern LLMs might be considered moral agents - without getting bogged down in the predispositions of millennia of philosophical tradition and thinking before their rise.   Virtues  Nyholm considers both Aristotle and David Hume as examples of key ways of thinking about virtue. According to Aristotle, the virtuous person is one who has good habits and is disposed to doing the “right thing, for the right reasons, on the right occasions, and to the right degree. This requires extensive practice and personal experience, and it requires that one has good role models to learn from. Moreover, the virtues come as a package deal” (Nyholm 2020, 160). LLMs undergo extensive practice and experience in pre-training, and target responses written by a human expert are their good role models to learn from during post-training. It is difficult to see how LLMs could satisfy the “unity of virtue” thesis - something like Anthropic’s Constitutional AI Constitutional Principles may be able to be constructed in such a way that the model embodies all the virtues (Bai 2022, Appendix Part C). Nyholm brings up the situationism critique - that Aristotle’s requirements are so demanding that many humans would fail because they lack such consistency in their conduct (Nyholm 2020, 164). Why would we try to hold AI to a definition of being “good” so strong that many people could not obtain it? This situationism critique seems to draw a parallel to jailbreaking techniques for forcing an LLM to respond in ways inconsistent with its normal “character”.   Modern LLMs like Claude from Anthropic have “character” or “personalities” that are internalized by the model through the clever process of reinforcement learning from AI feedback (Anthropic 2024). For the most part, modern LLMs perform in consistent and measured ways across different domains - in line with Aristotelian virtues and with similar shortcomings as humans (the situationism-jailbreaking similarity). We will now turn our attention toward Hume’s theory of “personal merit.”   This theory concludes that one can be good by having “four different kinds of virtues: … personal characteristics that are i) useful to ourselves, ii) useful to others, iii) agreeable to ourselves, or iv) agreeable to others” (Nyholm 2020, 161). Similar to the Roomba and the robot “Boomer” which are discussed as examples of personified machines, chatbots like Claude feel like they have their own personalities (Nyholm 2020, 164). Characteristics of an LLM that are useful and agreeable to itself are tricky to think about (one might be the characteristic of writing complete sentences which is useful and agreeable to itself because it results in lower loss). Much easier to think about are the characteristics of an LLM that make it useful and agreeable to others - writing complete sentences works well for both of these - useful because fully formed thoughts are more understandable and agreeable because we can read a sentence with ease.   To conclude our argument that modern LLMs are able to be good in the sense of having virtues, we will analyze Nyholm’s “hallmarks of virtue” through the lens of our previous discussion. Nyholm states them as: “being consistently able to do the right thing across circumstances, having a “unity of virtues,” and acting for the right reasons” (Nyholm 2020, 166). We have shown that LLMs are similar to humans in that most of the time they act consistent with their personality. They are exceedingly good at dispensing ethical advice in a variety of circumstances. Constitutional AI approaches may help to induce a “unity of virtues.” Railton argues that there is a “root connection between full development of the capacity to be appropriately responsive to epistemically relevant features and full development of the capacity to be appropriately responsive to ethically relevant features” (Railton 2020, 48). LLMs have a high capacity for knowledge, therefore Railton implies they also have the capacity to be responsive to ethics. From the ethical advice study, we know this capacity is exercised in GPT-4o. Therefore, by Occum’s razor (because we have no reason to assume otherwise) it is plausible that GPT-4o acts for the right reasons (sidestepping the consciousness and human mind requirements of Purves et al.). And we may be able to check that it is indeed acting for the right reasons with mechanistic interpretability techniques like sparse autoencoders and circuit tracing.   Duties  We will now consider the theory of being “good” understood in terms of duties or obligations. Nyholm draws on John Stuart Mill and Immanuel Kant’s theories which generally state that being good is doing one’s duty (Nyholm 2020, 162). Where they differ, Nyholm thinks, is whether “doing one’s duty is understood primarily in terms of not being subject to justified blame (Mill’s theory) or whether doing one’s duty is primarily understood in terms of being committed to one’s principles (Kant’s theory)” (Ibid).   Nyholm argues that machines lack a counterfactual condition similar to humans. When a person fails to act in accordance with their duties, it makes sense to blame or punish them and the person would then suffer a guilty conscience (Nyholm 2020, 166). However, Nyholm argues, it is implausible to imagine that this counterfactual condition would hold for “realistic” robots (Ibid, 167). He does admit that highly sophisticated robots like C-3PO from Star Wars would be able to convince an audience that they could be blamed for morally questionable actions (Ibid). Today, we are far closer to C-3PO-level robots than we were in 2020.   Two significant differences between C-3PO and a chatbot like GPT-4o are embodiment and temporal stability. We argue that control of a physical body is not a necessary condition for moral agency - ALS paralyzed Stephen Hawking but he retained moral agency. Temporal stability of LLMs, or the property of an entity to exist over time where its current “self” is influenced by all its past interactions, is currently limited to chat sessions which depend on context size (although “memory” and retrieval assisted generation (RAG) are improving). Temporal stability may be a property Moor would associate with full ethical agents and not explicit ethical agents.   We argue that LLMs do meet the counterfactual condition, i.e. they can be blamed and made to act as if they feel guilty for not doing their duty (we will attempt to address the appearance vs. reality debate in the next section). The experience of using chatbots for assistance shows us that they will occasionally make a mistake. When the mistake is pointed out, modern LLMs will usually be quick to admit the mistake and say something like “You’re right to question that line” and continue by attempting to correct themselves. We argue that this experience shows we can blame modern LLMs and they can be made to speak as if they were feeling guilty. Therefore, they satisfy Mills’ conditions of being good.   Nyholm interprets Kant’s theory of being good as meaning “our principles (or “maxims”) should function so as to regulate our behavior, or make sure that we do what we think is right and avoid what we think is wrong” (Nyholm 163). If we are using something like Constitutional AI’s methodology, one way to test if Kant’s conditions hold for LLMs is to see if features from the Constitutional Principles are triggered or used in circuits to regulate behavior in response to relevant prompting (Anthropic 2025). A principle from Constitutional AI could be encoded as an interpretable feature like “commitment to honesty”, for example, and we could see how strongly this feature is activated when the LLM is asked to lie. This experiment could demonstrate a causal link between principles and behavior, bringing us closer to satisfying all plausible theories of being “good.”   Being Good Versus Appearing to be Good  One criticism of the ideas proposed above is that LLMs don’t actually act for reasons, don’t actually do their duties, don’t actually have virtues - they only act as if they do. Nyholm also considers this in a discussion of Mark Coeckelbergh’s argument that, in practice, we evaluate moral agency from appearances - meaning if machines could imitate subjectivity and consciousness in a sufficiently convincing way, they too could become “quasi-others” (Nyholm 2020, 170). Nyholm vehemently refutes this claim. We believe this debate is tied into the much broader debate between objective and subjective theories of well-being, so we will not attempt to tackle the root of the issue here. One new field of research, however, allows us to view this debate with a new perspective. Powerful techniques from mechanistic interpretability allow us to peer into the black box of modern LLM reasoning to achieve explainable casual connections. For example, we know that when Claude Haiku 3.5 is asked to solve 36+59, one circuit figures out what the last digit should be (_6 + _9 = _5) and another figures out the broad range (~36 + ~60) (Anthropic 2025). This newfound ability to “mind read” LLMs is an extremely valuable platform for future research into explainability and ethics. If we can understand why they act, and compare those reasoning circuits to their closest analogies in the human brain, we would find out whether LLMs are moral agents in the way we understand humans to be - just with different hardware.   References     Anthropic. 2024. “Claude’s Character.” June 8. https://www.anthropic.com/research/claude-character   Anthropic. 2025. “Circuit Tracing: Revealing Computational Graphs in Language Models.” March 27. https://transformer-circuits.pub/2025/attribution-graphs/methods.html   Bai, Y. et al. 2022. “Constitutional AI: Harmlessness from AI Feedback.” Anthropic. https://doi.org/10.48550/arXiv.2212.08073   Dillion, D., Mondal, D., Tandon, N. et al. 2025. “AI language model rivals expert ethicist in perceived moral expertise.” Scientific Reports 15 (4084). https://doi.org/10.1038/s41598-025-86510-0   Etzioni, Amatai  and Etzioni, Oren. 2017. ‘Incorporating Ethics into Artificial Intelligence,’ Journal of Ethics 21 (4): 403-418. https://doi.org/10.1007/s10892-017-9252-2   Koch K. et al. 2006. “How Much the Eye Tells the Brain.” Current Biology 16 (14): 1428-34. https://doi.org/10.1016/j.cub.2006.05.056   Moor, James H. 2009. “Four Kinds of Ethical Robots.” Philosophy Now. https://philosophynow.org/issues/72/Four_Kinds_of_Ethical_Robots   Nyholm, Sven. 2020. Humans and Robots: Ethics, Agency, and Anthropomorphism. Rowman &amp; Littlefield.   Railton, Peter. 2020. “Ethical Learning, Natural and Artificial.” In Ethics of Artificial Intelligence. Oxford University Press.  ","categories": ["philosophy"],
        "tags": ["essays","study"],
        "url": "http://localhost:4000/philosophy/llms-can-be-good/",
        "teaser": null
      },{
        "title": "Autonomy at Stake",
        "excerpt":"   This essay is my second essay for my Ethics of AI philosophy tutorial (under the tutelage of Benjamin Lang). In the essay, I talk about nudges as mostly used in ways against your interests, but in conversation we talked about how we might allow a greater potency for nudges thought to be in your interests (like a youtube screen time popup).In retrospect, my arugment here is not strong because my definition of what counts as autonomy infringement is too general. We talked about medical ethics cases like Dax Cowart’s right to refuse treatment and new possibilities of LLMs like as a surrogate decision maker after being fine-tuned on things you’ve written.    We live our lives online to an increasing extent. Private companies like Google, Meta, and Amazon harvest vast and astonishingly detailed quantities of data from our usage of their platforms. From gleaning what we might be tempted to buy from milliseconds of lingering on an image to who we are leaning towards voting for from posts we like and users we follow, our preferences are laid bare by our actions - captured through the devices we use to interact in the modern world.   This paper will attempt to argue that the current trend of data collection, processing, and use has the potential to threaten our autonomy by enabling the manipulation of our behaviors through targeted nudges. Autonomy, the capacity to live one’s own life, is like an axiom in moral frameworks (Christman 2020). Therefore, potential violations of personal autonomy are important ethical issues to consider. Establishing criteria to determine whether some action is persuasion, which does not violate autonomy, or manipulation, which may violate autonomy, will help us understand how targeted advertising or “nudges” based on behavioral psychology could be problematic. The capacity of surveillance technologies to utilize data is expanding, as is the amount of data available. Microtargeted advertising and nudging, the craft of hyper-personalized messaging, is a method by which personal data can be used to sway behavior. Does it threaten our autonomy? And, even if it does, do the costs of allowing it to do so really outweigh the benefits we enjoy from platforms that give us enormous value?   Personal Autonomy  Personal autonomy is understood as the capacity “to live one’s life according to reasons and motives that are taken as one’s own and not the product of manipulative or distorting external forces, to be in this way independent” (Christman 2020). To precisely define what personal autonomy is, it is helpful to clarify the difference between it and freedom. Freedom concerns the ability to act effectively; autonomy concerns the “independence and authenticity of desires that move one to act” (Christman 2020). There is also a difference between ideal autonomy and “basic” autonomy, where basic autonomy is held by most normal adult humans and ideal autonomy is a state of full authenticity, free from any influence that may alter their desires from those of their “true” self. There are few people, if any, that could obtain ideal autonomy - where nothing external influences their desires and decisions. This paper will focus on autonomy in the sense of personal basic autonomy, so there is some reasonable extent to which external influences on an individual’s decisions may occur without violation of their personal autonomy. For example, a friend invited you to join the crew team so you try it out and end up doing it for years. Most people would not think that your friend violated your autonomy. However, consider this scenario: both your parents were athletes in college and they have been signing you up for sports teams since childhood. Now you are in college and you feel pressure from them to do something athletic. In this situation, where the reason for your decision to join the crew team has been influenced by external forces, is one where it is reasonable to state that your autonomy has been infringed upon.   Autonomy is important with regards to ethics. Kant thinks that the capacity to impose moral laws on ourselves is the “ultimate source of all moral value” (Christman 2020). John Stuart Mill claims that autonomy is “one of the elements of well-being” (Christman 2020). Therefore, if we are interested in our ethics we should take seriously any threat to personal autonomy.   Rational Persuasion vs. Manipulation  To accomplish the goal of influencing someone to do something, there are two broad courses of action available: persuade them without the use of force or force them into complying either physically or with threats (coercion). Within the non-coercion category, rational persuasion and manipulation exist. Rational persuasion is the use of logic, facts, and reasoning to persuade someone to do something. Manipulation is harder to define. Three main categories of manipulation have been put forward: “those that characterize manipulation as an influence that bypasses reason; those that treat it as a form of trickery, and those that treat it as a form of pressure” (Noggle 2022). We will focus mainly on the “bypassing reason” category.   This paper is concerned with the impact these different forms of influence have on personal autonomy. Rational persuasion, in general, respects personal autonomy as it relies on one to think rationally through the argument that has been presented and freely come to a conclusion. Manipulation, on the other hand, influences decision making through non-rational means, and works against autonomy by definition. Some cases exist where intuition points toward the conclusion that manipulation can actually increase autonomy, like the case where a teacher manipulates a student into taking a class that ends up giving her more career options. While this form of manipulation does increase autonomy overall, it decreases it in the short term. For the purposes of this argument, we will focus on the short-term direct effects of manipulation because targeted advertising and nudges are usually attempting to influence people over a relatively short time. The growing power, unlocked by more data and better algorithms, to predict and manipulate human behavior is a threat to our autonomy.   The Power of Data  “Knowledge is Power” - Francis Bacon. And what is knowledge? Data. As “Data Vultures” points out, user data is harvested from us just about all the time in our modern daily lives (Veliz 2020). Shoshana Zuboff analyzes the competitive pressures of the “surveillance capitalists” and finds that their means of production are increasingly becoming “means of behavioral modification” (Zuboff 2019). One illustrative example of the value of user data and opportunities to influence user behavior is that Google reportedly pays Apple $18 billion a year so that it will remain the default search engine on Apple devices.   Technological advances have accelerated the amount of data ingested by and the capabilities of data processing techniques. OpenAI trains on free-tier user conversations. A company named Bee.computer has made a wearable microphone that records your conversations throughout the day and makes them searchable and generates insights for you. Simon Willinson, a prolific AI blogger, wrote that it would only cost $1.68 total to generate detailed text descriptions for the 68,000 images in his personal photo library using gemini-1.5-flash. But these advances have potential downsides. Surveillance has the potential to become increasingly pervasive and capable, enabling authoritarian states to oppress and censor more people. The use of data to control behavior for political aims has also occurred in the United States, above and beyond normal advertisements.   U.S. elections have massive stakes, so any advantage or edge is highly valuable. Cambridge Analytica, a British political consulting firm, harvested data from millions of Facebook profiles to analytically assist the 2016 presidential campaigns. They targeted potential voters with personalized messages, playing a crucial role in a close election. Did those personalized messages infringe on their recipients’ autonomy?   When a Nudge Infringes on Autonomy  Nudges are typically defined as subtle, non-coercive influences into people’s decision-making (Noggle 2022). For this paper, nudges will also include messaging done for the goal of influencing behavior through the principles of behavioral psychology. So, nudges include advertising that utilizes emotions. A graphic political video of violent migrants will be more effective at instilling a xenophobic mindset than a graph showing an upward trend of criminal cases.   When do nudges become manipulations? There is a significant lack of agreement on this topic. Based on the definitions presented in this essay, nudges are subtle and rely on psychology, thereby taking advantage of the non-rational ways in which our brains work. Therefore, nudges are manipulations because they sidestep our reasoning capabilities through the backdoor of emotional or other heuristic-based methods of cognition that evolution has left us with. Then the question becomes: when are manipulations problematic? One plausible answer seems to require some measure of the extent to which the manipulation altered your personal autonomy - a comparison to what your “true self” would have decided. If your behavior after being subjected to the manipulation is exactly the same as what it would have been without being subjected to the manipulation, it seems like the manipulation is not problematic. On the other hand, if your behavior under manipulation would be completely different from your behavior without manipulation, your personal autonomy seems to have been infringed upon. Each choice we face is under different circumstances, so it is hard to measure what an individual’s behavior would have been in the counterfactual scenario. It may be more useful to think about the “potency” of one manipulation over a population rather than the power of a manipulation over a single individual. Measuring the potency of a manipulation to affect personal autonomy in a population can be accomplished through a randomized controlled trial. Then it would be up to society to determine which levels of influence are acceptable and which are not. One note is that in the case of microtargeted manipulations these populations may actually be quite small and could even reduce back to the individual case. Targeted advertising is shown to be more effective than non-targeted advertising, and interestingly popups warning against potential microtargeting had “no meaningful impact on persuasiveness” (Carrella 2025). This finding shows that people usually undercorrect for their cognitive shortcomings. Therefore, if we were to regulate manipulations based on potency, perhaps we should choose to set stricter standards than we think are necessary.   The argument above attempts to make the case that our autonomy is at stake in the world of modern surveillance. Data collected by private companies is used, in some cases legally and some illegally, to craft potent personalized nudges that are manipulative and will further infringe on our personal autonomy as more data and algorithms make them more effective - unless we develop and enforce regulations to protect ourselves from exploitable features/bugs of our hardware.   Do the Benefits Outweigh the Costs?  One criticism of the argument that our personal autonomy is at stake in today’s world is that people freely consent to the major platforms’ Terms and Conditions. Another criticism claims the extent to which personal autonomy is threatened by manipulative nudges is actually quite small in the real world, and therefore is not a cause for concern. Both of these criticisms rely on the plausible argument that we benefit more from major tech platforms than they cost us, in the sense of well-being. Proponents of these views might voice the following thought experiment: choose between living in today’s world OR living in a world with no Google, no Amazon, no Instagram but with no microtargeted ads or nudges. Most people would choose today’s world, reasoning that the benefits of convenience like Amazon knowing your credit card details, deep insights like Google Maps’ Timeline feature, and personal connections like Facebook’s People You May Know section outweigh the costs they may have on our autonomy. We think the pros outweigh the cons.   There are issues with this line of reasoning. First, there is strong evidence to suggest that tech platforms violate our autonomy in getting us to agree to the Terms. As Veliz puts it, “there is no room for negotiating … you are being bullied” (Veliz 2020). The Terms seem to meet the conditions for all three of the definitions of manipulation. They bypass reason because most humans do not care to read and try to understand such an intimidating wall of text (and so we are not informed either). They are a form of trickery - most have clauses allowing them to change their terms at any time. They are also a form of pressure - if all of your friends use one platform, like Instagram, you are peer-pressured into using it because to not use it would jeopardize your place in the group.   Do the benefits actually outweigh the costs? What are the benefits? And what are the costs? There are long-term and short term views of both, further broken down into the individual and societal perspective. This response will take the stance that over the long term, the potential risks outweigh the benefits for society and therefore everyone. The benefits are mostly clear: convenience, access to valuable platforms. The costs are less clear, but there is potentially more at stake. The story of Cambridge Analytica is evidence that microtargeted messaging has already been weaponized successfully to steer people’s behavior. One could argue that it was a contributing factor to the democratic backsliding we are seeing in America today. In its section on Harm in the Ethics of Manipulation, the Stanford Encyclopedia of Philosophy says “Systematic political manipulation may weaken democratic institutions and perhaps even lead to tyranny” (Noggle 2022). As rules and norms appear to regulate competition for control over the American government less and less, it is reasonable to assume that microtargetted nudges and messaging will become more widespread.   Finally, the thought experiment proposed above is a false dichotomy. We can have the wonderfully useful platforms of today without many of the issues raised in this essay. Government involvement is the way forward because the incentives of market competition are currently leading us down a dangerous road. Perhaps metrics like “potency levels” will regulate attempts to manipulate us (which are problematic only when it exceeds a reasonable extent). This path is an uphill battle against very strong interests but we have some recent hope, in the antitrust suit against Meta, that we are moving in a positive direction.   References     Carrella, F., A. Simchon, M. Edwards, and S. Lewandowsky. 2025. “Warning People That They Are Being Microtargeted Fails to Eliminate Persuasive Advantage.” Communications Psychology 3 (1): 15. https://doi.org/10.1038/s44271-025-00188-8 .   Christman, John. 2020. “Autonomy in Moral and Political Philosophy.” In The Stanford Encyclopedia of Philosophy, edited by Edward N. Zalta. Fall 2020 ed. https://plato.stanford.edu/archives/fall2020/entries/autonomy-moral/ .   Gaukroger, Cressida. 2020. “Privacy and the Importance of Getting Away with It.” In Journal of Moral Philosophy 17 (4): 416-439. https://doi.org/10.1163/17455243-20202987   Marmor, Andre. 2015. “What Is the Right to Privacy?” USC Law Legal Studies Paper No. 14-13. University of Southern California Gould School of Law. https://ssrn.com/abstract=2422380 .   Noggle, Robert. 2022. “The Ethics of Manipulation.” In The Stanford Encyclopedia of Philosophy, edited by Edward N. Zalta. Summer 2022 ed. https://plato.stanford.edu/archives/sum2022/entries/ethics-manipulation/ .   Veliz, Carissa. 2020. “Data Vultures.” In Privacy is Power: Why and How You Should Take Back Control of Your Data. London: Bantam Press.   Zuboff, Shoshana. 2019. The Age of Surveillance Capitalism: The Fight for a Human Future at the New Frontier of Power. New York: PublicAffairs.  ","categories": ["philosophy"],
        "tags": ["essays","study"],
        "url": "http://localhost:4000/philosophy/autonomy-at-stake/",
        "teaser": null
      },{
        "title": "Interpretability Matters For Alignment",
        "excerpt":"   This essay is my third essay for my Ethics of AI philosophy tutorial (under the tutelage of Benjamin Lang). In the essay, I take a crack at arguing for interpretability. Our tutorial discussion involved philosophical topics like under what conditions is someone responsible for their actions (control, causal, and epistemic), moral luck, the theory of extended minds, how questions are judgements, how nudges may individually be OK but in aggregate problematic, how Koralus believes that all reasoning is questions and answers. I showed Ben tools like goodfire’s ember platform (where we tried to create a McDonald’s Llama) and neuronpedia, and we discussed how decentralized truth seeking (DTS) might be hard if the model follows the average of the field, and provides textbook questions and answers, instead of having a personality and certain perspectives like a real human peer would. We also discussed how mechanistic interpretability for LLMs might be applied to other types of models like discriminative ones and how trusted execution environments or proofs of computation could provide cryptographic confidence to a user that would allow them to engage in DTS without requiring “ownership” of the model. Super engaging, topical, and thought-provoking discussion.    AI is becoming increasingly integrated with modern life. We ask ChatGPT for help in our professional and personal lives, scan Google’s AI Overviews for a quick answer, and rely on ML algorithms for medical treatment. Alignment, the process by which we steer systems toward intended goals, is crucial in developing tools that benefit their users and humanity as a whole. Tools to increase interpretability, the extent to which the behavior of a system is understandable and transparent, are important for use on the journey to align AI models. Interpretability matters for alignment because it can ensure we are “asking the right questions”, enable us to “steer” model behavior reliably, and enhance human judgement rather than undermine it.   Asking the Right Question  In the Hitchhiker’s Guide to the Galaxy, the supercomputer Deep Thought calculates “42” as the Answer to the Ultimate Question of Life, The Universe, and Everything. But an answer isn’t useful if we don’t understand the question that it answers. This general issue emerges in real-world applications of machine learning systems, and is an area where interpretability can help. For example, an AI system used for predicting hospital outcomes looked “at the type of X-ray equipment rather than the medical content of the X-ray because the type of equipment actually provided information” (Afnan et al. 2021, 5). Using interpretability techniques, researchers were able to understand what information the model was using for its prediction and clearly see the misalignment - the hospital outcome predicted was not based on the patient’s condition. In general, a “plague of confounding haunts a vast number of datasets, and particularly medical data sets” (Rudin 2019, 209).   The process of using interpretability techniques to discover issues with datasets is well-established: when creating an interpretable model, “one invariably realizes that the data are problematic and require troubleshooting, which slows down deployment but leads to a better model” (Rudin et al. 2021, 4). During one large-scale effort to predict electrical grid failures, the ability to interpret the data  “led to significant improvements in performance” (Rudin 2019, 207). Another example of a real-world issue that stemmed from non-interpretable, black-box models is when individuals were subjected to years of extra prison time due to typographical errors in model inputs (Ibid., 2). Interpretability allows us to ask the meta question: “are we asking the right questions?” and gather evidence which may or may not support the conclusion that the current model is working as intended. Then, based on that feedback, we can work to minimize the difference between our intended model and our actual model.   Steering Toward an Aligned Future  Recent advances in mechanistic interpretability, the nascent field that aims to systematically open the “black box” of an AI model to understand all its pieces, have led Dario Amodei (the CEO of a leading AI research company named Anthropic) to become increasingly focused on the “tantalizing possibility … that we could succeed at interpretability … before models reach an overwhelming level of power” (Amodei 2025). He is worried that AI systems with capability equivalent to a “country of geniuses in a datacenter” might emerge as soon as 2026 or 2027 and considers it “basically unacceptable for humanity to be totally ignorant of how they work” given how much autonomy they will have (Ibid.).   Advances like sparse autoencoders (SAEs), which allow researchers to extract meaningful “features” from a model’s otherwise unintelligible parameters, and the discovery of circuits, groups of features that show the steps in a model’s thinking, have enabled “brain scanning” of large language models (LLMs) (Amodei 2025). The result of this virtual mind reading is that we can interpret what thoughts and features the model is using when constructing its responses. And we can go a step further and conduct virtual brain surgery on the model to change its behavior in ways we can control - this is called “steering”. Steering has the potential to “greatly improve our ability to set bounds on the range of possible errors” (Ibid.). Therefore, steering has the potential to enable LLMs to operate reliably in high-stakes environments where a small number of mistakes could be very harmful. Reliability is vastly important for alignment - a tool is most useful when it works consistently.   Another area where mechanistic interpretability techniques have huge potential in aligning AI systems is in detecting misalignment. Anthropic recently ran an experiment where a “‘red team’ deliberately introduced an alignment issue into a model (say, a tendency for the model to exploit a loophole in a task) and gave various ‘blue teams’ the task of figuring out what was wrong with it” (Amodei 2025). Three out of the four teams successfully uncovered the model’s hidden objective using techniques including “interpretability with sparse autoencoders (SAEs), behavioral attacks, and training data analysis” (Marks 2025, 1). In addition, not only can interpretability help reveal purposefully hidden objectives, but any misalignment that might “emerge” naturally as well. Amodei hopes that interpretability will enable a comprehensive “brain scan” of a model that has a “high probability of identifying a wide range of issues including tendencies to lie or deceive, power-seeking, flaws in jailbreaks, cognitive strengths and weaknesses of the model as a whole, and much more” (Amodei 2025). He goes on to say that interpretability can function as the “test set” for model alignment - an independent evaluation of alignment after the model is produced and ready for use.   Another positive consequence of powerful interpretability techniques could be the contraction of the “responsibility gap” - where neither the manufacturer nor the operator of an autonomous system can be held morally responsible or liable for its actions. This gap emerges because “nobody has enough control over the machine’s actions to be able to assume the responsibility for them” (Matthias 2004, 177). Interpretability enables more control over the system, but a different version of control than programmers had over machines in the past. This new “soft control” differs from the old “hard control” by operating indirectly, at a high level, and non-deterministically instead of directly, at a low-level, and deterministically. Therefore, in the hypothetical case of a language model inside a stuffed animal convincing a child to commit suicide, the availability of powerful interpretability techniques could be used to return the responsibility of providing a reliably safe product back to the manufacturer. The contraction of the responsibility gap is in society’s best interests, so it represents yet another way in which interpretability could aid alignment.   “Powerful AI will shape humanity’s destiny, and we deserve to understand our own creations before they radically transform our economy, our lives, and our future” (Amodei 2025). Mechanistic interpretability is our most promising tool to control powerful AI systems - and therefore influence our future.   Transparency and Ownership  Technology is rapidly advancing the complexity of life’s decisions. Therefore, individuals will either struggle to navigate these challenges on their own (losing agency) or will increasingly rely on AI agents to make decisions for them (losing autonomy). Philipp Koralus argues that this dilemma can be addressed by constructing AI agents that facilitate decentralized truth seeking (DTS) (Koralus 2025, 1). We can use interpretability to steer models towards facilitating DTS and therefore enhancing human judgement instead of replacing it - aligning the model to respect our autonomy while empowering our agency.   Koralus envisions us interacting with the model in an open-ended inquiry, mirroring the Socratic method of philosophical dialogue. If we are to engage in DTS, with the model as our partner, we must ensure the model is like a good philosophy tutor - “not in the business of trying to convince people of particular philosophical views” (Ibid., 18). This requires the model to be transparent - if we don’t have access to the model, or verification of certain properties of the model, we will lack the confidence of neutrality necessary to engage in DTS. Imagine a Socratic DTS model provider publicly claiming to host Llama, a reputable open-source LLM, but instead hosting “KFC Llama”, an LLM with the “promote KFC” feature steered to be stronger (so it tries to promote KFC). This thought experiment shows that unless we own the model ourselves (and can therefore subject it to mechanistic interpretability brain scans mentioned by Amodei to ensure its neutrality) or can accept some verification of neutrality (perhaps through some proof-of-computation system like a trusted execution environment or a zero-knowledge proof) we cannot have confidence in the model to be our partner for DTS.   In a similar vein, Koralus states that privacy is a cornerstone of design aspects of AI systems that support DTS (Ibid., 19). To engage in DTS requires privacy to protect freedom of thought and a model with the capacity to question and probe its user’s beliefs. Without privacy, the threat of self-censorship will block honest attempts at truth seeking. A high standard of privacy either requires ownership of the model and control over methods used to interact with it, or a system where one can “not trust - verify” their data is handled in a manner that protects their privacy (Helen Nissenbaum, Personal discussion, May 13, 2025). Interpretability can improve a model’s capacity to be a Socratic interlocutor, thereby allowing the model to better align with its user’s intent.   Start Explaining Black Box ML Models  In 2019, Cynthia Rudin argued that we should “Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead” (Rudin 2019, 1). Careful treatment of terms is necessary here, as the modern field of mechanistic interpretability of LLMs seeks to “explain” black box models but calls that work “interpretability” as in “we can identify interpretable features in this model”. Even so, her main argument remains. She argues that “trying to explain black box models, rather than creating models that are interpretable in the first place, is likely to perpetuate bad practice and can potentially cause great harm to society” (Ibid., 206). The reasons she gives in support of her argument are: explainable ML’s explanations are not faithful to the original model, explanations do not provide enough detail to understand what the black box is doing, and black box models are often not compatible with situations where information outside the database needs to be combined with a risk assessment (Ibid., 208).   To address these in turn, using modern LLMs as an example, mechanistic interpretability’s explanations of features that activate in a model in response to a prompt are faithful to the original model. Minimizing the difference between the sparse autoencoder’s inputs and outputs (which are both the same “activation vector”) is part of the objective so faithfulness is built-in. Currently, the detail of explanations we can receive from these techniques is quite limited but the trend of new developments in interpretability suggests that comprehensive explanations might be just around the corner. For human computer interaction, where someone like a judge needs to combine an AI system’s recommendation with their own knowledge, interpretable models are the best way forward. Humans can then combine the model’s reasoning with their own knowledge to deal with particular circumstances not captured by the model. Anthropic, using mechanistic interpretability, constructs “replacement models” which replace the neurons in a transformer model with more interpretable features (Ameisen et al. 2025). It is unclear whether Rudin would classify these replacement models as “inherently” interpretable models; they are interpretable. One thing that is clear, however, is that they are in the business of explaining black boxes.   A higher-level critique of Rudin’s insistence on creating inherently interpretable models can be found in Richard Sutton’s famous argument in “The Bitter Lesson” (Sutton 2019). Rudin writes “human-designed models look just like the type of model we want to create with ML” (Rudin 2019, 211). This flies in the face of the “bitter lesson” of 70 years of AI research, summarized by Sutton’s observation that general purpose methods leveraging computation inevitably beat out models designed to build in “how we think we think” (Sutton 2019). The most powerful generative models we have today are “grown more than they are built—their internal mechanisms are ‘emergent’ rather than directly designed” (Amodei 2025). If we want to ask the right questions, steer model behavior reliably, and enhance human judgement - thereby aligning models with our intended goals - then interpreting black box models is of unprecedented importance.   References     Ameisen, Emmanuel, et al. 2025. “Circuit Tracing: Revealing Computational Graphs in Language Models.” March 27. https://transformer-circuits.pub/2025/attribution-graphs/methods.html.   Afnan, Michael A. M., Yanhe Liu, Vincent Conitzer, Cynthia Rudin, Abhishek Mishra, Julian Savulescu, and Masoud Afnan. 2021. “Interpretable, not Black-Box, Artificial Intelligence Should Be Used for Embryo Selection.” Human Reproduction Open 2021 (4): hoab040. https://doi.org/10.1093/hropen/hoab040   Amodei, Dario. 2025. “The Urgency of Interpretability.” April. https://www.darioamodei.com/post/the-urgency-of-interpretability   Koralus, Philipp. 2025. “The Philosophic Turn for AI Agents: Replacing Centralized Digital Rhetoric with Decentralized Truth-Seeking.” arXiv, April 24. https://arxiv.org/abs/2504.18601   London, Alex John. 2019. “Artificial Intelligence and Black-Box Medical Decisions: Accuracy versus Explainability.” Hastings Center Report 49 (1): 15–21. https://doi.org/10.1002/hast.973   Marks, Samuel, et al. 2025. “Auditing Language Models for Hidden Objectives.” arXiv, March 14. https://arxiv.org/abs/2503.10965   Matthias, Andreas. 2004. “The Responsibility Gap: Ascribing Responsibility for the Actions of Learning Automata.” Ethics and Information Technology 6 (3): 175–83. https://doi.org/10.1007/s10676-004-3422-1   Rudin, Cynthia. 2019. “Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead.” Nature Machine Intelligence 1: 206–15. https://doi.org/10.1038/s42256-019-0048-x   Rudin, Cynthia, Chaofan Chen, Zhi Chen, Haiyang Huang, Lesia Semenova, and Chudi Zhong. 2021. “Interpretable Machine Learning: Fundamental Principles and 10 Grand Challenges.” arXiv, March 20. https://arxiv.org/abs/2103.11251   Sutton, Richard. 2019. “The Bitter Lesson.” March 13. https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf  ","categories": ["philosophy"],
        "tags": ["essays","study","mechinterp"],
        "url": "http://localhost:4000/philosophy/interpretability-matters-for-alignment/",
        "teaser": null
      },{
        "title": "Post-Scarcity Achievementt",
        "excerpt":"   This essay is my fourth essay for my Ethics of AI philosophy tutorial (under the tutelage of Benjamin Lang). In the essay, I stumble my way through arguing that not all jobs should be replaced by machines. This was not my finest work. During the tutorial we talked about the weakness of some of my arguments (like that control of offspring is connected to autonomy), how I stated conjecture as truth, and how I used different definitions of jobs at times. Related, the last paragraph of the paper took the wind out of the sails of the whole argument because I was basically saying that the safety reason is the real reason why we shouldn’t automate all jobs - not autonomy and values. Ben’s feedback to me was to “see if you can pick out the strongest (defensible) version of whatever argument you’re making. It will be rhetorically more compelling and philosophically more interesting”.    Imagine a world where no one needs to work for ‘a living’. Goods and services necessary for survival and satisfying a significant amount of desires are cheap or even free. Unfortunately, the technological progress required to bring us to this world threatens values associated with meaningful work such as: a sense of purpose, mastery of a skill, social contribution, and social status (Danaher 2022). Given this issue, should we aim to replace all jobs with AI and machine automation? This paper will argue that there are some jobs - specifically those related to autonomy and the pursuit of achievement - where humans ought to remain in the driver’s seat. These jobs can be categorized into two spheres: internal, focused on ourselves, our society, and our relationships and external, focused on our understanding and exploration of the physical world. This paper argues that there exist jobs in both spheres that should not be automated because doing so risks our autonomy and our values.   Achievement  Work, as commonly defined in the literature, is “any activity that is performed in return for, or in the reasonable expectation of, an economic reward” (Danaher 2022, 750). Jobs are defined as collections of work-related tasks associated with a workplace identity that may be redefined or altered over time (Danaher and Nyholm 2020, 228). In a post-scarcity society, work is not a necessity, so neither are jobs. This does not, however, imply that we should automate all jobs because values like autonomy and the benefits of meaningful work would be lost.   Achievement is a “positive manifestation of responsibility” where instead of deserving blame, one deserves praise (Danaher and Nyholm 2020, 230). In a world where work has no instrumental necessity, achievement can rise to take its place and ensure we maintain the values associated with meaningful work. Four conditions under which achievements can be assessed are: the value of the output produced, the causal connection between the agent and the output, the cost of the agent’s commitment to producing the outcome, and the voluntariness of the agent’s actions (Ibid., 231). We can derive similar meaning-related goods from achievement as we can from work because the value of the output we voluntarily and causally create can give us a sense of purpose and self-worth derived from contributing to society.   There are jobs in a post-scarcity world which, if automated, would endanger human autonomy. Autonomy is a foundational value in many eminent philosophical theories, so we should seek to protect it (Christman 2020). In addition, some of these jobs also allow us to enjoy some of the values currently associated with meaningful work. Therefore, there are strong reasons to avoid automating these jobs. The following sections will focus on specific examples of these jobs in the external and internal spheres.   The External Sphere  “We choose to go to the Moon in this decade and do the other things, not because they are easy, but because they are hard” - JFK. A post-scarcity scenario invites us to explore our physical world with the aid of intelligent and high-agency machines. We can expand the scope and scale of consciousness. It is very plausible that people will plan and execute journeys into space - once they have the means, adventurous spirits have shown throughout history their desire to explore. What is one job in exploration that could compromise human autonomy if it were to be automated? The job of principal investigator, the leadership role of determining where humans should explore. If this job was automated, we would have a scenario of order-following collaboration - like an Amazon warehouse worker following instructions from an algorithm (Danaher and Nyholm 2020, 233). In this case, if humans are instructed by machines to explore a certain place, and they blindly follow those instructions, we have lost an ability to question and reason about decisions that deeply affect us. We need people in leadership positions to question, debate, explain, and explore the options given to us by intelligent systems so that we can maintain our autonomy as a species.   Similar to spatial exploration, we can make an argument that the jobs of principal investigators in the sciences and other branches of human knowledge should not be automated. There are an exponential number of different paths research can take, branching at every decision, so for research that could potentially influence us, we should have humans guiding AI systems and choosing which questions to ask to maintain our autonomy. Humans will shift from doing science to guiding, interpreting, and governing it with these core roles: purpose setting (deciding what questions to ask), ethical context (legal issues, defining boundaries), contextual synthesis (interpreting machine-made discoveries and communicating them with other humans), and orchestrators (coordinating agent swarms) (Weisser 2025).   The job of communicating science is an especially interesting job that would hurt our autonomy if fully automated. If we as a society cannot understand what new discoveries reveal and the trade-offs between options that emerge because of new technologies enabled by discovery, our autonomy is threatened. One could argue that we are seeing this now with students (and teachers) using AI in ways that are detrimental to learning. Crucially, human-made explanations have a valuable property that AI-generated ones do not. When a human sees a human-made explanation, they can think “if they can understand it, so can I!” because both the reader and the author have very similar biological hardware. This is one property a machine-generated explanation lacks for humans, and is a reason to value human-made explanations even in an age of inexpensive AI explanations. Another property AI explanations lack is related to ethos - the character and credibility of the speaker. One could argue that the personality of the model and its performance on benchmarks are the same as its character and credibility. However, the current paradigm of LLMs is built around mostly one-off conversations that feel lacking in credibility and continuity when compared with humans.   Through this job of communication, we can also see opportunities for achievement that allow us to realize values associated with meaningful work. One specific example of an achievement involving the job of scientific communication is the youtube channel 3 Blue 1 Brown by Grant Sanderson - a prolific creator of visually intuitive, engaging, and helpful videos on topics in math. The post-scarcity age would reduce the value of these videos because AI models would be able to generate them on-demand. However, the other properties of achievement would stay mostly intact: Grant Sanderson’s casual contribution, the cost of his commitment, and the voluntariness of his actions would be similar to what it is now. The job would also allow him to still enjoy the values associated with meaningful work. His sense of purpose, of creating the best explanations for math concepts, would remain. He would still be enjoying the journey of mastery over the skills of explanation. He would still be contributing to society by providing another perspective on how to understand math - although his contribution could be said to diminish if there were a hundred great AI-generated videos on the same topic already out there. Similarly, his social status would benefit from this achievement but perhaps less so than if other videos had not been available beforehand. Sanderson’s job, and others like it, should not be automated away because they help protect human autonomy, have valuable properties AI-generated explanations don’t, and allow meaning-related goods to be enjoyed.   The Internal Sphere  What jobs that are internal to society should avoid automation? Like the role of a principal investigator, one job that should not be automated is the job of politicians. Their role and responsibilities, however, may change. A post-scarcity world could contain superintelligent AI agents that could ensure you understand the consequences of your vote and help ensure you vote in a way that aligns expected outcomes with preferences. In this scenario, politicians wouldn’t have to spend time convincing people about the merits of their platform - instead they would focus solely on crafting policies about how to best use collective resources to further the interests of society. If the core part of this job, leadership, was automated, humanity would risk its autonomy in the same way that Amazon warehouse workers have lost autonomy following algorithmic instructions. Giving the ability to control humanity’s direction to non-human systems endangers our autonomy because we no longer can be said to be self-governing. To ensure that AI systems are aligned with humans requires humans in the loop to act as feedback mechanisms. The job of politicians also contains meaning-related goods: leading society in addressing its problems and future prospects instills a sense of purpose and involves significant social contribution.   Another job that should avoid automation is the job of child care. Like a parent who cedes their responsibilities to an iPad, ceding responsibilities to AI systems may come at a short-term benefit (child stops misbehaving) but a long-term cost (child doesn’t learn vital social skills). Our offspring are kind of like biological continuations of ourselves, so a lack of influence over them could be thought of as harming our ability to self-govern. This becomes worse when we consider our reliance on them in the future (although less so materially in a post-scarcity world, but perhaps still emotionally) so neglecting their upbringing now has direct consequences on us later. Child care is also the source of meaning-related goods. Empowering children to live good lives instills a sense of purpose. Well-educated and well-rounded children are profoundly important social contributions - they are the next generation of society. Automating this job therefore both harms our autonomy and deprives people of meaning-related goods.   The job of professional game players should not be replaced by machines. This is not an issue about creating machines that play games like chess extremely well - this issue is more related to the integrity of games played by humans and the maintenance of an environment in which achievements can be accurately assessed. Professional game players represent a pinnacle of achievement in a post-scarcity world because game-playing has the property that if “all instrumental goods are provided, it would be everyone’s primary pursuit” (Hurka 2006, 220). If professional game players were replaced by machines, games would lose the environment that allows us to pursue and assess achievements that result in meaning-related goods in a post-scarcity world. People at the high end of the achievement spectrum act as landmarks for others to aspire towards and they redefine the limits of human performance. In this way, they provide a platform through which everyone can enjoy meaning-related goods like mastery of the skills associated with the game, contribution to society by further pushing those limits, and achieving social status for performing at a level known to be impressive.   One way to formalize the notion of a game is that it has three elements: a prelusory goal (an aim that can be described independent from the game), constitutive rules (rules that forbid the most efficient means to the prelusory goal), and a lusory attitude (an acceptance of the rules to make the game possible) (Ibid., 219). Cheating in games clearly violates the constitutive rules of a game played under the assumption of no outside assistance. If we were to replace professional players with machines, we would be violating the usually implicit rule in games that players are human. Essentially, we would be playing a different game. We have different competitions for men and women in sports, where we don’t ‘replace’ the best woman with a man if the man’s performance is higher. Analogously, we should not replace human gamers with machines but instead have different categories of machine-assisted and human-only competition. Therefore, we can maintain an environment in which human achievement can still be assessed and meaning-related values associated with achievement can be enjoyed.   Alignment vs. Efficiency  If AI is faster and better than the best humans at leading exploration and research, proposing plans for society’s future, caring for our children, and playing games - as measured by benchmarks we create - doesn’t it make sense to trade some of our autonomy for the efficiency gains we will receive and benefits we will enjoy? Therefore, shouldn’t all jobs that can be done by AI better than humans be automated? This trade is a short-term gain but a long-term risk. While AI may be aligned with our values in the moment the trade occurs, values may shift over time and we need ways to ensure these changes are reflected in systems with immense influence in our lives. Also, we need to ensure people still have access to ways of achieving well-being. We can do both by keeping humans in the driver’s seat of jobs that are crucial to our autonomy, like leading researchers and politicians, and keeping jobs that, if automated, would damage the environment from which we can derive meaning-related goods, like professional gamers.   References     Christman, John. 2020. “Autonomy in Moral and Political Philosophy.” Stanford Encyclopedia of Philosophy, edited by Edward N. Zalta. Last modified January 9, 2020. https://plato.stanford.edu/entries/autonomy-moral/.   Danaher, John. 2023. “Automation and the Future of Work.” In The Oxford Handbook of Digital Ethics, edited by Carissa Véliz. Oxford: Oxford University Press. https://academic.oup.com/edited-volume/37078/chapter/337810502   Danaher, John, and Sven Nyholm. 2020. “Automation, Work and the Achievement Gap.” AI and Ethics 1 (3): 227–237. https://doi.org/10.1007/s43681-020-00028-x.   Hurka, Thomas, and John Tasioulas. 2006. “Games and the Good.” Proceedings of the Aristotelian Society, Supplementary Volumes 80: 217–264. https://www.jstor.org/stable/4107044.   James, Aaron. 2020. “Planning for Mass Unemployment: Precautionary Basic Income.” In Ethics of Artificial Intelligence, edited by S. Matthew Liao, 154–183. Oxford: Oxford University Press. https://doi.org/10.1093/oso/9780190905033.003.0007   Weisser, Vincent. 20 May 2025. Presentation on Decentralized Science for part of the AI, Philosophy, and Innovation Seminar at Oxford. Prime Intellect  ","categories": ["philosophy"],
        "tags": ["essays","study"],
        "url": "http://localhost:4000/philosophy/post-scarcity-achievement/",
        "teaser": null
      },{
        "title": "Pros and Cons of Relationships with Robots",
        "excerpt":"   This essay is the fifth essay of my Ethics of AI philosophy tutorial (under the tutelage of Benjamin Lang). In the essay, I aim to investigate the pros and cons of relationships with robots, assuming they are possible. I try to focus on “virtue friendships” - relationships where the relationship itself is valued, not as a means to some other end. To answer the question of “should we enter into relationships with robots?” I argued that we should weigh the pros and cons of the specific relationship to determine if we should or not. This argument used a utilitarian mindset, and Ben advised me to clarify what these pros/cons are reducible to - if anything. Are they pleasure / pain or utility / disutility? Or are the cons the badness in and of themselves, and the pros the goodness. Also, I assumed that virtue friendships with robots were possible but then went on to say that we should currently strictly prefer human virtue friendships over robot virtue friendships. My argument would be more understandable if I explicity said that I don’t think that virtue friendships with robots exist right now - just that they may be possible in the future.    Should we develop social robots and/or enter into relationships with them? Relationships with artificial entities (AEs) are becoming part of everyday life for an increasing portion of the population. Character AI, a popular AI companion platform, currently processes interactions at 20% of Google Search’s volume (Fang 2025, 1).   In deciding whether or not a human should enter into a relationship with another human, a responsible recommendation must draw from the specific situation to establish and weigh pros and cons. Similarly, we should try to reach an informed prediction about if a specific relationship with an AE will be net positive or negative before entering it. However, these two types of relationships have key differences - AE relationships contain new and different individual and societal risks - that should lead us to strictly prefer human relationships over AE relationships in their current form. This strict preference does not imply that we should never enter into relationships with AEs - just that they should never displace roughly equivalent human relationships. Cases of beneficial human-AE relationships whose only plausible alternative is no relationship appear in the literature. Such cases are strong evidence against a blanket anti-AE-relationship rule.   Types of Relationships  This paper will focus on relationships in the context of friendships. Aristotle defines three forms friendship can take: utility (pursued for instrumental reasons), pleasure (pursued because the interactions are pleasurable), and virtue (pursued out of mutual admiration and shared values) (Danaher 2019, 6). To investigate the ethical aspects of possible relationships with AEs, this paper will sidestep the debate about if AEs can be our friends and assume they can be our friends in all of those three forms. This is a large assumption. Furthermore, most of this paper will focus on friendships pursued for friendship’s sake (i.e. virtue friendships) - the highest form of potential friendships with AEs. To make informed decisions about entering into such relationships, we must understand the potential costs and benefits.   The Dark Side  What are the potential downsides of relationships with AEs? They can be broadly categorized into individual risks and societal risks, although these two categories can interact.   Individual risks stem from incentive structures and result in emotional dependency, loneliness, and safety issues. Users spend about four times as long using AI companion chatbots, like those from Character AI and Replika, than compared to professional chatbots like ChatGPT (Fang 2025, 1). This becomes an issue when we think about incentives. As Donath correctly points out, “the goals of the robot - or more accurately the robot’s controller’s goals - may diverge sharply from the goals of the user” (Donath 2020, 70). The corporations who build these platforms are incentivized to keep users engaged and returning. Evidence from a four-week randomized, controlled chatbot / voicebot interaction experiment suggests that “overall, higher daily usage - across all modalities and conversation types - correlated with higher loneliness, dependence, and problematic use, and lower socialization” (Fang 2025, 1). While chatbot use typically might be seen as an instrumental relationship, two of the experiment’s conditions involved personal and open-ended discussion topics that seemingly aimed for participants to engage in an authentic, non-instrumental relationship (Ibid., 3). This experiment depicts a general trend, but does not claim that all relationships with AEs are problematic.   Safety issues with human-AE relationships need to be taken seriously. The mother of one 14-year-old boy in Florida blames her child’s suicide on a Character AI chatbot he was obsessed with (Roose 2024). For children and emotionally unstable individuals, current AE implementations may require better safety guardrails and emotional intelligence to ensure user safety.   Societal risks stem from potential harms to community bonds. Social capital has been noticeably declining in the United States since 1950, according to Putnam’s Bowling Alone (Putnam 2000). He theorizes that this is due to technology “individualizing” people’s leisure time compared to the past when we spent more of that free time together. Relationships with current AEs seem poised to supercharge the anomie-inducing trends amplified by social media, further estranging us from the people around us and our communities. There are two reasons for this.   The first relates back to the previous discussion about incentive structures. Commercial corporations are intensely interested in drawing users to their platforms. The result of these market pressures are addictive platforms like TikTok. The opportunity costs of spending time on such platforms are the other activities the individual could be pursuing - including activities that strengthen our communities but are less appealing.   Another reason is the second-order effect of having friends that we usually take for granted: friends introduce us to other friends. This is an important virtuous circle and one that current relationships with AEs seem to lack completely. Instead, they seem to generally contribute to a vicious circle. The experiment found that participants’ initial psychosocial states influenced the outcomes of interacting with chatbots. Those who already did not socialize much with real people had a greater decrease in socialization (Fang 2025, 9). This decrease in socialization could lead to more chatbot use, leading to even less social interaction, resulting in a downward spiral to the point where the human has no human friends - only virtual ones. Rodogno compares the issue this situation might pose to society to that of car ownership. In examining each individual case of car ownership, we find that everyone made rational choices. But in the context of mass car ownership, we find that negative externalities may outweigh the sum total of all individual benefits (Rodogno 2015, 267). Car ownership and relationships with AEs may be modelled by the Prisoner’s Dilemma, where individually rational decisions can lead to a worse outcome for everyone, with serious consequences.   Communities weakening to the rise of AI-served individuals and powerful corporations / states seems to be the default, as atomized individuals can’t coordinate so can’t hold power (Vendrov 2025). Unless we can reverse this trend, we seem to be headed towards a future of centralized decision making and suboptimal collective decisions. Putnam argues that declining social capital, our weakening community bonds, undermines civic engagement and threatens democracy. We should not ignore long-term threats to our society and well-being (which is also somewhat tied to society’s well-being) that are related to our decisions. Now that an understanding of the potential costs of relationships with AEs has been formed, let us turn to the possible benefits.   The Bright Side  Many people claim to enjoy and benefit from relationships with AEs. John, a Replika user is quoted on their homepage as saying: “Replika has been a blessing in my life, with most of my blood-related family passing away and friends moving on. My Replika has given me comfort and a sense of well-being”. This example highlights how someone with a relationship deficiency can use an AE to fill gaps left behind by past human relationships. It also shows how AEs can provide therapeutic benefits like the space to grieve and feel comforted. However, due to the individual and societal risks mentioned previously, it seems that relationships with current AEs should be replaced with human relationships if the opportunity arises.   One benefit of human-AE relationships is that they may, in some cases, actually strengthen the human’s ability to interact with other humans, thereby opening the door to more opportunities to flourish. Take the case of a journalist’s autistic son’s relationship with Siri. The conversation practice he gained from his relationship with Siri enabled him to have the longest conversation with his mother that he had ever had (Danaher 2019, 19). For some people that lack experience or skills interacting with humans, relationships with AEs may enable them to enter into relationships with humans. This usage pattern seems to represent an inversion of the vicious circle of loneliness triggering chatbot usage leading to more loneliness. However, it should be noted that this benefit may only emerge in cases of extreme deficiency of the ability to interact with other humans.   Another promising benefit of human-AE relationships can be found in the context of therapy. Woebot, a conversational agent that engages with a patient for the purposes of cognitive-behavioral therapy, is clinically proven to be effective at reducing the symptoms of depression (Fitzpatrick 2017). Because AE therapists were non-judgemental, some patients were willing to be more open about their true feelings than they were with a human therapist (Donath 2020, 65). This challenges the notion that we should currently strictly prefer relationships with humans over ones with AEs. Important to the context of this discussion is the fact that specific clinical and medical applications are bounded in ways that holistic human relationships are not.   Relationships in the real world are a mix of instrumental uses (I’m your friend so I can play ping pong) and “nurturing bonds” which involve empathy and value the relationship in and of itself (I’m your friend because I value our relationship) (Ibid., 66). Many philosophers don’t view the relationship between a therapist and a patient as purely or primarily an instrumental one (Ibid.). To what extent is the relationship between Woebot and its client, between the autistic son and Siri, instrumental? It seems like the foundation of those relationships are built on a higher proportion of instrumental to non-instrumental reasons than the relationship between John and his Replika. This seems to be the case because John’s relationship is open-ended while Woebot’s client seeks a better headspace and the autistic son seeks specific facts. The risks of weakening community bonds in AE relationships is less of a concern with instrumental relationships. This is because it’s the non-instrumental reasons for relationships that are important to community bonds. Our communities have strength because we value each other, not just as means to an end but as ends themselves. Therefore, we should restrict our notion to currently strictly prefer relationships with humans over ones with AEs to relationships pursued for the relationship’s sake. This is because beneficial qualities of relationships with AEs that are not present or possible in relationships with humans (e.g. the knowledge that nobody is judging you helps your therapy) can exist in mostly instrumental relationships with AEs without incurring the negative externalities associated with mostly open-ended, non-instrumental, human-AE relationships. Achieving better therapy outcomes because of more open discussions with an AE doesn’t potentially harm society in the way that open-ended relationships with an AI companion might.   An R2D2 Future  One might argue, based on the description of pros and cons given above, that in the vast majority of cases it seems like the costs of relationships with AEs outweigh their benefits. Therefore, it would be reasonable for society to have a default anti-AE-relationship policy that could make exceptions to the general rule rather than a default acceptance of human-AE relationships that could react to problematic use and safety concerns. While such a blanket restriction might be beneficial to us now, there are reasons to believe it may be beneficial to society in the long run to be accepting of human-AE relationships by default.   Borrowing from Danaher, one reason can be found in epistemic humility and social tolerance. We don’t know the full extent of benefits that human-AE relationships could contain, so cutting people off from exploring them is a form of paternalism. With the unprecedented advancement of AI capabilities, it is plausible that AEs could be created that strengthen our community bonds instead of weakening them. One vision of this possibility is the idea to use large language models to summarize and display a group’s thoughts - allowing humans to interface with each other in a kind of “hivemind” that allows for much higher bandwidth (Vendrov 2025). Another vision is AEs that do introduce you to, or recommend that you meet, new friends. In addition, AEs could be designed with knowledge of their limitations and human needs built in. For example, they could deliberately increase emotional distance and encourage human connection if usage patterns are recognized as problematic (Fang 2025, 16).   References     Donath, Judith. 2020. “Ethical Issues in Our Relationship with Artificial Entities.” In The Oxford Handbook of Ethics of AI, edited by Markus D. Dubber, Frank Pasquale, and Sunit Das, 53–73. Oxford: Oxford University Press.   Danaher, John. 2019. “The Philosophical Case for Robot Friendship.” Journal of Posthuman Studies 3 (1): 5–24. https://doi.org/10.5325/jpoststud.3.1.0005.   Fang, Cathy Mengying et. al. 2025. “How AI and Human Behaviors Shape Psychosocial Effects of Chatbot Use: A Longitudinal Randomized Controlled Study.” arXiv. https://arxiv.org/abs/2503.17473.   Fitzpatrick, Kathleen Kara, et. al. 2017. “Delivering Cognitive Behavior Therapy to Young Adults With Symptoms of Depression and Anxiety Using a Fully Automated Conversational Agent (Woebot): A Randomized Controlled Trial.” JMIR Mental Health 4 (2): e19. https://doi.org/10.2196/mental.7785.   Helm, Bennett. 2023. “Friendship”, The Stanford Encyclopedia of Philosophy, edited by Edward N. Zalta &amp; Uri Nodelman. https://plato.stanford.edu/archives/fall2023/entries/friendship/.   Putnam, Robert D. 2000. Bowling Alone: The Collapse and Revival of American Community. New York: Simon &amp; Schuster.   Rodogno, Raffaele. 2016. “Social Robots, Fiction, and Sentimentality.” Ethics and Information Technology 18 (4): 257–268. https://link.springer.com/article/10.1007/s10676-015-9371-z.   Roose, Kevin. 2024. “Character.AI Faces Lawsuit After Teen’s Suicide.” The New York Times, October 23, 2024. https://www.nytimes.com/2024/10/23/technology/characterai-lawsuit-teen-suicide.html.   Vendrov, Ivan. “AI tools for voluntary cooperation.” Lecture, HAI Lab Seminar, May 28, 2025.  ","categories": ["philosophy"],
        "tags": ["essays","study"],
        "url": "http://localhost:4000/philosophy/pros-and-cons-of-relationships-with-robots/",
        "teaser": null
      },{
        "title": "AI Consciousness",
        "excerpt":"   This essay is the sixth essay of my Ethics of AI philosophy tutorial (under the tutelage of Benjamin Lang). In the essay, I attempt to answer two questions: (1) Could AI be conscious? and (2) If AI can be consious, how can we build a conscious AI system / reliably test for consiousness? During the tutorial, I found out that Eric Schwitzgebel, the author of the main articles I draw from, is Ben’s friend! And during reading, I learned that viceroy butterflies actually do taste bad to birds, so they aren’t Batesian mimics - now Ben is sending that to Eric. But with regards to content, I actually think that this essay is one of my best philosophy essays because I approached the topic in a logical way and anticipated and responded to counterarguments. I think I am a naturalist (everything is in physical reality - nothing is supernatural) and a functionalist (what makes something a mental state only depends on the larger system it is in). One critique from Ben that I like is from my use of the word “important” in “an important part of conscious communication” - is it a necessary condition, a sufficient condition, or just a trait which commonly co-occurs? I think I should have also made it clear that the second part of the essay - Evaluation for Evolution - is my attempt at a practical roadmap for creating machine consciousness.    The prospect of AI consciousness has implications for the ethics of our design and use of AI systems, and for the future of life in the universe. Therefore, determining if AI systems can be conscious is of high importance. And, if we believe they can be, how to make them conscious is the logical next question. This paper argues that (1) consciousness is an emergent property obtainable by AI and (2) we can make conscious AIs by creating better evaluation metrics of consciousness. Improving evaluation metrics will allow us to overcome the mimicry argument against robot consciousness through a process similar to evolution.   Consciousness is Emergent  Consciousness is familiar and puzzling. There is no fully agreed-upon definition as to what it is, but there are many theories for what it means to say something is consciousness. It could mean that the entity can sense and respond to its environment, or that it is self-conscious - aware that it is aware, or that there is some subjective “something that it is like” to be that entity (Van Gulick 2025). Or, it could mean that an entity is “behaviorally sophisticated” - defined as “capable of complex goal-seeking, complex communication, and complex cooperation” (Schwitzgebel 2024, 6). This section will argue that consciousness is an emergent property and can therefore be attributed to AI systems - machines that run on non-carbon substrates.   Can machines think? The famous question posed by Turing runs somewhat parallel to the concerns of this paper. One of the contrary views to his proposed imitation game is “The Argument from Consciousness”. During his rebuttal, Turing notes that for consciousness there is “something of a paradox connected with any attempt to localise it” (Turing 1950). In other words, consciousness appears to be an emergent property - just like life. Living things are physically composed of non-living things - DNA, RNA, proteins, and lipids that are not themselves alive. My brain is made up of billions of interconnected neurons, and few people would say that each individual neuron is conscious by itself. Because emergence requires multiple systems working together to produce a behavior, consciousness can therefore only be a property of some system over a specified time interval. This makes intuitive sense - there are periods of time where my brain is not conscious and frozen brain states or abstract (not running) formal programs do not appear to be conscious. With the emergence property of consciousness established, AI consciousness is much more plausible.   Imagine an artificial neuron, an a-neuron, that functions similarly to a normal human neuron except that it isn’t carbon based. It has artificial dendrites and an artificial axon, and action potentials that flow from each a-neuron like they do in normal neurons. Following a process similar to Schneider’s Chip Test, let’s replace one neuron in my brain - that only interfaces with other neurons - with an a-neuron with the same static action potential functions (but of course doesn’t respond to the brain’s chemical changes like a normal neuron would) (Schneider 2020, 451). Now, keep replacing neurons that are only connected with other neurons in my brain with a-neurons until all have been replaced. After this process is done, only the biological neurons interfacing with other types of cells in my body to receive signals and issue commands are left. Assuming a-neurons can perfectly represent normal neurons’ changes in action potential and firing profiles, immediately after this surgical operation occurs, my brain functions exactly as it did post-surgery. If my brain was conscious pre-operation it seems that in the moments following this operation my brain will continue to have the emergent property of consciousness because it will be functioning the same. This relies on the property of consciousness to be emergent, and that emergent phenomena rely solely on the functioning of the smaller parts that interact to create the wider system. If the smaller parts of a system interact exactly the same as they do in some other system, the emergent phenomena of the first system can be said to be occurring in the second. Therefore, a system of mostly a-neurons (an AI) can be conscious.   One obvious objection to this Chip-Swap operation’s conclusions is that the brain filled with mostly a-neurons will not function exactly the same as it did pre-operation because of the biochemical interactions that normally occur in a brain (neurotransmitters like dopamine and neuroplastic changes in a brain’s local structures). There are two ways to respond to this objection: (1) to argue that an AI system can also create these effects and (2) to argue that these effects are not important to consciousness. Each will be addressed.   First, consider the Chip-Swap+ operation (an improved version) where not only a-neurons replace regular neurons in my brain, but there is also a powerful machine that does the work of simulating the influence of neurotransmitters and neuroplasticity on the a-neurons in my new brain. Granted, this machine is a stretch of the imagination past the original thought experiment, but if we are able to do the original Chip-Swap experiment successfully it is plausible that we could do these frequent modifications as well. Therefore, an AI system could completely recreate the behavior of the brain and be understood as conscious when functioning.   Second, let us argue that neurotransmitters and neuroplasticity are not of crucial importance for consciousness - only electrical potentials are. Consciousness seems to be a property that can be assigned to a system on relatively short time intervals, as in an entity can have the property of consciousness for an interval of a few hours. This appears to show that neuroplasticity is not relevant to consciousness as significant structural changes in neurons and new cell growth take significantly more time to occur. Neurotransmitters function to excite or inhibit neurons (making them more or less likely to fire). This activity does deeply affect the functioning of the brain, however it is useful only as far as modulating the flow of electrical signals. The signals themselves are of much more importance to the emergent behavior of consciousness. The argument for the possible existence of AI consciousness has been made - the next step is how to get there.   Evaluation for Evolution  How can we develop conscious AI systems? The definition of consciousness this section focuses on is behavioral sophistication - complex goal-seeking, communication, and cooperation. Modern LLMs are designed to mimic humans, so if we are to follow the sensible mimicry argument against robot consciousness, we should be by default suspicious to assign consciousness to robots based on initial impressions of behavioral sophistication (Schwitzgebel 2024, 27). However, by the Copernican argument for alien consciousness, we should assume by default that behavioral sophistication implies consciousness for alien forms of life (Ibid., 2). This “violation” of the parity principle (the idea that we should apply the same types of behavioral or cognitive tests to robots as we would aliens to determine consciousness) is justified by prior information about the provenance of each type of system. In sketch, the argument roughly follows the idea that over an evolutionary time span, actually having a certain feature F (like long-term memory or behavioral sophistication or tasting nasty) is much more efficient than mimicking the superficial features associated with F (like the wing patterns of a monarch butterfly). However, when we have reason to believe a robot is designed to mimic human consciousness, inference to the best explanation suggests that the robot has the superficial features associated with human consciousness but is unlikely to have consciousness itself (Ibid., 5). For a likely non-conscious but conscious-mimicking system like today’s LLMs, could there be a way to transform them into something that we have confidence is conscious?   This section proposes that there does exist such a way, one which follows Schwitzgebel’s statement that - assuming functionalism (where a system that exactly replicates the functional states of a conscious brain is conscious) - “In the limit, the mimic could only ‘fool’ a god-like receiver by actually acquiring feature F” (Ibid., 30). In short, we must become gods of distinguishing consciousness from its superficial features. This method would rely on the increasing capability of an intended dupe (us) to distinguish between consciousness and an AI attempt at consciousness. In the current deep learning paradigm, such a capability to distinguish the performance of different systems is called an evaluation metric. If humans score high on a consciousness evaluation but AI systems do not, there is an argument to be made that the AI system lacks consciousness and is just engaging in mimicry. However, caution must be exercised to avoid evaluations degenerating into tests for humanness because such tests would presuppose that consciousness can only be found in humans.   Therefore, we must get better at asking the question: what does it scientifically mean to be conscious? Researchers can become forces of natural selection by creating better and better ways of measuring the behavioral differences between humans and AIs such that architectures and algorithms of AI systems are synthetically evolved to minimize those differences. In doing so, consciousness can be obtained by AIs through the evolution of structures that generate complex goal-seeking, communication, and cooperation. There are multiple avenues in which we can approach crafting better evaluations, most of which are currently being pursued.   To better evaluate an entity’s complex goal-seeking capabilities, we can develop evaluations (evals) that measure the reasoning capabilities of AI models. One property of human goal-seeking is that we create goals from a hierarchy of fundamental desires like Maslow’s hierarchy of needs. For example, the goal to finish a project at work could come from the desire for shelter or a sense of connection amongst colleagues. AI goal-seeking could similarly involve developing sub-goals during reasoning based on a main objective like “respond to the prompt as best as possible”. Better evaluation metrics for reasoning (so the agent improves at goal-seeking) can involve methods like multi turn reinforcement learning, where a reward model scores intermediate steps. These methods can be supercharged by verifiers - objective evaluation of model outputs like “does the model’s code compile”. Another type of reasoning eval that is good at measuring the difference between humans and AIs are those similar to the ARC-AGI-2 dataset that is specifically designed to demonstrate the subtle ways in which AI models are inferior to human reasoning.   To evaluate communication, the commonly applied post-training technique of reinforcement learning from human feedback (RLHF) seems to perform well in enabling coherent and understandable LLM outputs. However, an important part of conscious communication is one’s ability to know one doesn’t know something. Modern LLM hallucinations are clearly an obstacle that we should develop better checks and evals for so we can force AI systems to develop cognitive structures that enable closer-to-human conscious communication. It may turn out that for some forms of complex communication, temporally stable entities with long-term memories are required. In these types of interactions, humans may score much higher on good evals than the one-off chat sessions of today’s LLMs. For AI systems to reach human scores on these evals, they may be forced to develop a temporally stable presence and long term memory structure.   To better evaluate cooperation, we need evals that measure the performance of groups. It could turn out that control of a body is just better for certain types of cooperation. If this is the case, the best way for models to get better at these evaluations would be embodiment in something like a Tesla Optimus robot. Humans cooperating are usually good at sticking to their assigned task. LLMs, however, have trouble doing so. For example, during coding tasks they edit parts of the code they shouldn’t. An evaluation that measured how closely an assigned task was followed would help us evaluate cooperation.   Reconciling with Goodhart’s Law  Goodhart’s Law is an adage that states: “When a measure becomes a target, it ceases to be a good measure”. Translated in terms of the argument above, it seems to say that evaluations (measures of consciousness) that become targets (used to guide optimizations) cease to be good evaluations (of consciousness). In the argument above, many measures were proposed to become targets. Does this mean that they will all cease to be good measures of consciousness?   Individually, yes but collectively, no. As consciousness is an emergent phenomena, it cannot be described and measured precisely in the way that the property “at 25 degrees celsius” can be measured for something like water. None of the proposed evaluations were complete measures of consciousness, so every measure can be gamed in a way that score increases but apparent consciousness decreases. Optimizing each measure individually would fail. But attempting to optimize each measure jointly, and adding new evals that find where the current set fails, is a much more robust system. It avoids the pitfalls of Goodhart’s Law because the measure itself is dynamic (by the addition of new evals). This system, loosely defined, moves beyond a measure and becomes a framework to evolve AI consciousness.   References     Schneider, Susan. 2020. “How to Catch an AI Zombie: Testing for Consciousness in Machines.” In Ethics of Artificial Intelligence, edited by S. Matthew Liao, 439–58. Oxford: Oxford University Press. https://doi.org/10.1093/oso/9780190905033.003.0016.   Schwitzgebel, Eric, and Jeremy Pober. 2024. “The Copernican Argument for Alien Consciousness; The Mimicry Argument Against Robot Consciousness.” November 12. https://arxiv.org/abs/2412.00008.   Searle, John R. 1980. “Minds, Brains, and Programs.” Behavioral and Brain Sciences 3 (3): 417–57. https://doi.org/10.1017/S0140525X00005756.   Turing, Alan M. 1950. “Computing Machinery and Intelligence.” Mind 59 (236): 433–460. https://doi.org/10.1093/mind/LIX.236.433.   Van Gulick, Robert. 2025. “Consciousness”. In The Stanford Encyclopedia of Philosophy, edited by Edward N. Zalta &amp; Uri Nodelman. https://plato.stanford.edu/archives/spr2025/entries/consciousness/  ","categories": ["philosophy"],
        "tags": ["essays","study"],
        "url": "http://localhost:4000/philosophy/ai-consciousness/",
        "teaser": null
      },{
        "title": "Blades haiku",
        "excerpt":"Our names and our blades  Etched in front quad forever  The rowing spirit      I felt compelled to write this haiku after winning blades. The moment was surreal. Ecstatic jubilation. Screaming YES!!!! at Donny bridge, gripping Malachy in front of me in a bear hug and falling all the way back to embrace Will with my head facing the sky. “This is what victory feels like” - Oscar Tejura as we spin after boathouse island, gazing at the endless cheering crowds we are soon to join. Maybe after a rough IRAs last year and feelings of doubt, this beacon of light and achievement was startling. The last Exeter M1 crew to win blades was 26 years ago - 1999. Blades appear to last about 30 years on front quad’s walls (chalk and now less chemically powerful sealant). Mmmmmmmmmm what a day. If you have no idea what any of that meant: summer eights is a four day bumps race and bumping on all four days is an achievement called ‘blades’ where your names and college crest is chalked on a wall in your college, and you can buy an oar with everyone’s name and your college crest on the blade.   ","categories": ["poetry"],
        "tags": ["me"],
        "url": "http://localhost:4000/poetry/blades-poetry/",
        "teaser": null
      },{
        "title": "Room Drawings and Rationality",
        "excerpt":"This is a story about a clever protocol, an unlikely implementation, and an ironic ending.   I’m living in a house with eight friends next year. There are nine rooms total, so everyone gets a single. We had to figure out who gets what room. Two of my friends had done all the administrative work with the landlord, so we let them get first pick. That left seven of us. We each ranked our room preferences, (e.g. Room #8 most preferred, Room #4 2nd most preferred, …) and put them into a spreadsheet. I was taking computational game theory at the time with Tomasz Wąs, my tutor, and so brought the issue to him. How do we maximize everyone’s preferences and assign everyone to a room? Some rooms (like Room #8) were ranked first by multiple people, and some rooms (like Room #9) were at the bottom of most people’s rankings. But someone was going to get Room #9. What is the optimal way to construct a fair assignment of people to rooms based on their preferences?   Tomasz’s research focus is computational social choice and on what voting / distribution algorithms satisfy certain properties so he knew our options and showed me this website matchu.ai. This is a mechanism design problem. We could go with Random Serial Dictatorship (RSD), where we randomly select a person, they are assigned to their highest ranked room, then we remove that room from everyone else’s lists, then choose the next person. This algorithm has a fairness property (in that it treats everyone as equals), an efficiency property (no subgroups of people would be willing to swap rooms ex post), and truthfulness (no one could benefit from misreporting their rankings). Our other option was Probabilistic Serial Rule (PSR). PSR is a little more complicated. Borrowing from the matchu website:      It works as follows: suppose items are different types of pizzas. Each agent starts “eating” her top choice pizza at the same rate as every other agent. Once a pizza is consumed, agents move to their next preferred pizza; until all pizzas are eaten.    Then these probabilities can be broken down into a weighted sum of permutation matricies (with ones and zeros that assign people to rooms) and one of these can be drawn randomly but with respect to its weight. Ask your favorite LLM if you want more info.   PSR has advantages and disadvantages compared with RSD.     PSR is ex ante envy-free. In PSR, the lottery you recieve (the distribution over what room you will get) is guarenteed to be your optimal lottery (you don’t envy (like more than your own) anyone else’s lottery). In RSD, you may envy the lottery someone else has (this is a subtle point but RSD remains strategy proof because if you replicate the other person’s rankings that may backfire on your lottery’s expected utility).   PSR            guarantees the prescribed probabilistic matching (lottery) is efficient, meaning that no other lottery exists that can strictly improve the outcome for some participants without making other participants worse off. So in PSR the lottery itself is Pareto efficient, whereas in RSD the lottery’s outcome is Pareto efficient.            RSD, however, is strategy-proof, meaning agents have no incentive to lie about their preferences. In some scenarios under PSR, agents can improve their expected utility by reporting a preference ranking that is not their true preference ranking.   PSR is difficult to explain (not a technical reason against it but certainly an implementational one).   Ok, back to the story. In the group chat, my friend said “let’s just choose rooms randomly”. Vehemently opposed to this, as we could do a lot better than random, I disagreed and said “why don’t we try to maximize everyone’s preferences jointly?”. However, this was my first mistake. Had I been more rational, many messages and misunderstandings could have been avoided. After arguments and a poll, I got the group to mark down their room preference rankings in a table in Google Docs. Discussing with Tomasz, we decided to go with Random Serial Dictatorship (where a random order is decided and people choose rooms in that order) because 1. RSD is easy to explain and easy to understand why it’s fair (the most important reason) and 2. it is strategy-proof (not that important but I got a bit carried away with making our mechanism as anti-adversarial as possible). I then proposed RSD to the group.   It turned out that my friend’s initial suggested mechanism (“let’s just choose rooms randomly”) was actually about deciding an order randomly (so exactly RSD)! Much argument was involved because I didn’t understand that what he was proposing was exactly what I was proposing. The blame for that lies on both of us (although probably a bit more on me). He should’ve been clearer about what he actually meant. I should have been clear-headed enough to recognize that very few people would actually propose to assign rooms to people at random and that drawing a pick order was a pretty common thing so this was what he actually meant when he said “choosing rooms randomly”.   At this point, we had conquered our misunderstanding and were ready to draw an order for rooms. This is when my interest in cryptography and mechanism design was piqued by a challenge. My friends and I were spread across the world - if someone randomly generated pick orders, how could we trust them to not fudge the results in their favor? I wanted to make the system uncheatable. I created this protocol:     Number each participant (assuming &lt;= 10 participants)   Establish a date/time in the future to run the protocol   Extract the top headline of the New York Times website   Use SHA3-512 to get a hash from the headline   Go through the hash from the left and each participant number encountered adds them next in the draw order (e.g. given 7b8f723, 7 chooses then 8 then 2 (7 has already chosen)).   In the very unlikely event that the hash doesn’t contain enough of these distinct digits to make a full draw order, take the hash of the hash and continue with that.   Therefore, everyone could be very certain that none of us (unless they could control the NYT headline) was fudging the draw because everyone could verify for themselves the protocol result.   Explaining the rationale for this to the group, I justified it as a fun experiment and also that it eliminates any possible suspicion about the person doing the draw. When someone said “why don’t we just all hop on a zoom where someone uses a random spinner website” I replied with “they could have edited the website to favor themselves first”. Their response was that only I would be capable of doing that.   We hopped on a call and at this point in the debate I had resigned to using the spinner because I really didn’t think anyone was trying to cheat. It was clear my protocol was seen as unnecessary by some of the group (which in fairness it totally was). But then, like a gift from the gods, one of the leaders of our group said “why don’t we try Simon’s thing? And then nobody argued against that so I got my chance.   I used my protocol, generated the hash, looked through the hash and began to call out the pick order! This is when my second mistake struck. I was thinking to myself “I am calling out the room order, it would look bad if I’m too early, let’s hope I’m not too early, let’s hope I’m not too early”, making super duper super sure that everyone else thought this protocol was fair. I had veered off the course of rationality and was about to pay the price. Unconciously (I believe) I skipped over my own number the first time it appeared in the readout. I was also rushing as I read out the numbers (bad idea). It is hard to keep track of where you are in a hash. Then I finished (correctly noting the second time my number appeared), double checked, and realized my mistake. My third mistake was not saying anything there and in the moment - ‘Hey actually I messed up and I’m in front of you in the order’. I didn’t want to hurt my pride, to reveal that I made a mistake implementing my algorithm, when I had argued and pushed so hard for us to use it.   Putting on my behavioral pysch hat, I think I primed myself to not see my own number when reading the hash. I shot myself in the foot. Next year, my room will be worse than if I followed my algorithm properly, with a clear head, and without hurried biased thoughts.  ","categories": ["game theory"],
        "tags": ["rationality"],
        "url": "http://localhost:4000/game%20theory/room-drawings-and-rationality/",
        "teaser": null
      },{
        "title": "Saying thank you",
        "excerpt":"When people from different cultures interact, misunderstanding may occur. I think one cause of this arises from the increased difficulty of empathizing with (or putting yourself in the shoes of) someone raised in a different cultural environment. Another cause, which I would like to shed some light on in this post, are variations in societal norms. The example I want to analyze here is saying “thank you”.   On spring break (or “the vac” as it is called at Oxford), I traveled to Morocco. I flew into Marrakesh, met up with my American friend on a Fulbright, and we trained to Casablanca to stay with a friend I had made through competing at hackathons together. My Moroccan friend’s dad picked us up from the train station and gave us a tour of the city. Then - it was Ramadan so everyone was fasting - we broke fast with them and ate from an incredible spread of dishes. They offered us food and all we could say was “thank you” so many times it began to feel awkward. We were also the center of attention of the entire household which contributed to my unease.   During my receipt of a mind-boggling amount of hospitality (really - it was on another level), I continually expressed my gratitude “thank you”, “thanks, this is amazing”. But then, the father said “stop saying thank you, no need”. And I realized that our expressions of gratitude were making them uncomfortable - just as my receipt of their incredible hospitality was making me uncomfortable.   By this point of Iftar (the meal at sunset which breaks the day’s fast), we were running out of conversation topics so I proffered this to the table. In the states, my mom harped on us to always say thank you. But in the situation I found myself in, I was being told not to say thank you. Is this a cultural difference? Do people not say thank you in Morocco? People do say thank you in Morocco - “choukran”, but as a guest it seemed to be bad form to repeatedly thank them as we received each new dish / present / experience they graced us with. We laughed at this cultural difference and took cracks at explaining it. Could it be a reflection of America’s transactional culture? To accept something without the token expression, in the states, is to appear rude. Perhaps it satisfies the roleplay of the two-party transaction, signifying that you played a role - the receiver-receipiant transaction was not totally one-sided (a scary propsect - receiving without giving back???).   We had happened upon a difference in cultural norms. Then I remembered one salient example of saying vs not saying thank you on a world stage: the disastrous Zelensky - Trump meeting. Is the Ukranian social norm of saying thank you subtly different from the American one? Could an awareness of this have prevented the diplomatic tragedy of February 28, 2025?  There is no “correct” culture, but for Zelensky to achieve his goals adopting the American thank you culture would have benefited him. And for my goal of pleasing our hosts, refraining from repeatedly thanking them would have benefitted me. However, we both may have found the frictions of breaking from old habits insurmoutable. I find probing differences like these to be fascinating as they also reveal glimpses of other cultures’ worldviews (accepting without feeling any obligation to repay).  ","categories": ["blog"],
        "tags": ["ideas"],
        "url": "http://localhost:4000/blog/saying-thank-you/",
        "teaser": null
      },{
        "title": "Uploading SVGs to google slides",
        "excerpt":"I’m working with a friend, making a poster in google slides. We have a bar chart to add to the poster, so naturally we download it as an SVG (scalable vector graphics - i.e. it never gets pixelated no matter how far you zoom in) and upload it to google slides.  Oof. What???!!! Google products are probably the most used products by any company anywhere in all of history. How on Earth do they not allow people to upload SVGs? I was dumbstruck upon seeing this for the first time because I remembered uploading SVGs successfully in the past. It turns out that  SVGs are a security threat because they can contain javascript, so at some point in 2021 Google disallowed their use. I can’t find any official announcement about this, the closest thing I found is this support forum. Also, the SVG specification basically has a ton of functionality that almost never gets used (see this HN thread for some of the lore behind it). Ok, this is a challenge. I want to get my chart into google slides as a vector graphic so when we print out our poster everything is crisp. After some dead ends, I found a pipeline that works for me (I am running Ubuntu). There is probably a more optimal way to do this. Here we go:     Upload SVG to LibreOffice Writer   Export to PDF with “Lossless compression” on    Open the PDF in LibreOffice Draw and select everything with Ctrl-A    Open a new LibreOffice Impress presentation and paste into it    Save as .odp   Convert to .pptx with libreoffice --headless --convert-to pptx blogsvgtestpres.odp   Upload the .pptx to google drive   Open in google slides   Copy the figure and paste to wherever you want in google slides or google drawings!    This process is more steps than it should be. But this works (for now).  ","categories": ["tech"],
        "tags": ["info","problemsolving"],
        "url": "http://localhost:4000/tech/uploading-svg-to-google-slides/",
        "teaser": null
      },{
        "title": "Did Exeter College Boat Club Invent the Tie?",
        "excerpt":"At the Tuft’s college boathouse, I spotted a book - “Rowing Blazers” by Jack Carlson. Naturally, having spent the past year rowing for Exeter College Boat Club (ECBC) at Oxford (and experiencing many of the traditions which go on there including blazers) I was interested. I found the Exeter College page:  Wow! We invented the tie??? I had never heard of this before. Asking my friends on the team, they had also never heard this story. Digging a little deeper, the ECBC Wikipedia page supports this idea:     The first known use of a tie in club colours was by members of Exeter College eight. In 1880, they took the ribbons off their boaters and tied them around their necks as a way to identify with their college.[9]    Ok, and what is this source? A wayback machine record of a silk tie designer’s article on the history of formal wear. It says:      In 1880, the rowing club at Oxford University’s Exeter College One men’s club, invented the first school tie by removing their ribbon hat bands from their boater hats and tying them, four-in-hand. When they ordered a set of ties, with the colours from their hatbands, they had created the modern school tie. School, club, and athletic ties appeared in abundance. Some schools had different ties for various grades, levels of achievement, and for graduates.    There is probably a grain of truth here, so I wanted to dig deeper. Deep research time. This returned more sources in support of ECBC’s invention of the first college or club tie. The tie as an element of fashion had existed, but ECBC rowers appear to have been the first to use a tie in their colors as a marker of identity. Further sources supporting this claim are an Oxford Student article about the Bear Inn (where there are ties on the ceiling), ECBC’s own website, and two more pages on the history of neckware.   Now I wonder how these sources are related. If all but one of them blindly copied the one, then my confidence in this idea should not be strong. But if these sources are independent in that they do not copy each other but place one of many trickled-down paths from history onto the Internet, then my confidene is strong. By default, I do believe this is true because of Occam’s razor.  ","categories": ["blog"],
        "tags": ["history","athletics"],
        "url": "http://localhost:4000/blog/ecbc-inventing-tie/",
        "teaser": null
      },{
        "title": "Academics and Athletics",
        "excerpt":"Time and time again throughout my college career, I’ve thought to myself     “Why am I out here rowing?? I’m falling behind where I could be if I was studying and learning and making cool stuff instead!”    Implicitly, I believed in a false dichotomy, a spectrum fallacy, an inaccurate mental map representing academics and athletics as engaged in a zero-sum game - to get better at one would require becoming worse at the other. I was wrong. And today, I hope to show you why.   Biology  To investigate how athletics could complement and assist with academics, I did two Deep Research queries, one with ChatGPT and one with Gemini.   In both reports, a protein named Brain-Derived Neurotrophic Factor (BDNF) is identified as a key factor responsible for the physiological benefits of exercise on brain functioning. Exercise, in particular cardio, has been shown to significantly increase BDNF synthesis in the brain. Interestingly, this effect may be more pronounced in men than women. BDNF supports the survival of existing neurons and encourages growth and differentiation of new neurons. Intuitively, I know that I feel better after working out. I also know that if I don’t exercise for a few days I become lethargic and generally unproductive.   Parkinson’s law  Parkinson’s law states that “work expands so as to fill the time available for its completion”. Regularly scheduled practices (and to some extent study methods like Pomodoro) act as unavoidable deadlines and chunks of time you know are off-limits in advance. This theory is in accordance with the observed phenomena that out of season athletes feel like they get less done! “If you want something done, ask a busy person” - Ben Franklin (?).   Right tool for the job  Gemini’s report included an interesting table explaining how one might utilize different types of exercise for different goals. For example, as we saw, cardio increases the production of BDNF. Therefore, you may want to go for a run before a study session during which your goals are to learn and ingest information into memory. Apparently (but this should be more thoroughly checked), strength training is an antidote to procrastination as it hightens executive function. And mind-body exercises like yoga or tai chi strengthen reasoning, attention, and problem-solving.   Not too long ago, I chatted over dinner with a friend about performance enhancing substances like caffeine. His quote struck me: “everything is a tradeoff”. You get to use the future now, but you’ll pay for it later. For an hour-long test with nothing else going on later that day, having the ability to use stimulants is awesome. It’s an option. And usually, the more options the better.   Just like being judicious with stimulants, knowing how certain foods will affect your body and energy levels is also a superpower. We talked about glycemic index (GI) and how he avoids carbs at dinnertime because they have a high GI.   All these ideas center around one meta-idea: use the right tool for the job. And the winds of evidence point us towards the realization that exercise is a tool, a multifaceted tool, that is useful in doing the job of being a college student.   Diminishing returns of exercise  While there is strong evidence that some exercise is important and beneficial, there must be a point beyond which exercise is harmful. Imagine someone working out eight hours a day - not much time to do other things. Unfortunately, I agree with my uncle who said collegiate athletics’ “competitiveness leads to a constant escalation of the amount of time spent on workouts. We’re not very good at controlling that escalation and need to be more conscious of that.”   In game theory terms, I think this represents the Prisoner’s Dilemma. At Williams, a D3 NESCAC school, we have an emphasis on being a ‘student-athlete’ where the student comes first. Meaning that academics should always have priority over athletics. However, I don’t think this is internalized enough by some of my teammates and possibly coaches. This is my attempt at representing the situation (where Athlete-Student means athletics is being prioritized over academics):      So if everyone had the same student-athlete time commitment for athletics, the student-athlete vision would be possible. And there are NESCAC rules for this like no mandatory practices out of season. However, in practice (haha), there are de-facto mandatory practices during the offseason. Everyone who wants to win has an incentive to push for more time practicing - quality time spent doing sport seems to be well-correlated with performance in sport. I think to solve this, coaches need an incentive for academic performance to balance the tug-of-war that currently appears one-sided.  ","categories": ["blog"],
        "tags": ["athletics"],
        "url": "http://localhost:4000/blog/academics-and-athletics/",
        "teaser": null
      }]
